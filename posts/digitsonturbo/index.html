<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>DigitsOnTurbo: Optimizing Large Number Arithmetic Operations | Subhrajit Das</title>
<meta name=keywords content="DigitsOnTurbo,Large Number Arithmetic,Optimization,Vedic Mathematics,Addition,Subtraction,Multiplication,GNU Multiprecision Library"><meta name=description content="This project is part of my M.Tech Thesis at IIT Gandhinagar, exploring optimizations for arithmetic operations on large numbers."><meta name=author content="Subhrajit"><link rel=canonical href=https://iamsubhrajit10.me/posts/digitsonturbo/><meta name=google-site-verification content="iy00If8zyMomM1J1jOJO-pvNlypdUMxUXuRKEaxSn5k"><link crossorigin=anonymous href=/assets/css/stylesheet.fc220c15db4aef0318bbf30adc45d33d4d7c88deff3238b23eb255afdc472ca6.css integrity="sha256-/CIMFdtK7wMYu/MK3EXTPU18iN7/MjiyPrJVr9xHLKY=" rel="preload stylesheet" as=style><link rel=icon href=https://iamsubhrajit10.me/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://iamsubhrajit10.me/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://iamsubhrajit10.me/favicon-32x32.png><link rel=apple-touch-icon href=https://iamsubhrajit10.me/apple-touch-icon.png><link rel=mask-icon href=https://iamsubhrajit10.me/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="DigitsOnTurbo: Optimizing Large Number Arithmetic Operations"><meta property="og:description" content="This project is part of my M.Tech Thesis at IIT Gandhinagar, exploring optimizations for arithmetic operations on large numbers."><meta property="og:type" content="article"><meta property="og:url" content="https://iamsubhrajit10.me/posts/digitsonturbo/"><meta property="og:image" content="https://media.licdn.com/dms/image/v2/D4D03AQE-pgjjAFUfvQ/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1724069567965?e=1729728000&amp;v=beta&amp;t=WN9GYnbdp5lxYeVoj63bsjhJTf746I2lkprVOvY7cAA"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-08-15T13:30:59+05:30"><meta property="article:modified_time" content="2024-09-23T13:30:59+05:30"><meta property="og:site_name" content="Subhrajit Das - M.Tech CSE Student at IIT Gandhinagar"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://media.licdn.com/dms/image/v2/D4D03AQE-pgjjAFUfvQ/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1724069567965?e=1729728000&amp;v=beta&amp;t=WN9GYnbdp5lxYeVoj63bsjhJTf746I2lkprVOvY7cAA"><meta name=twitter:title content="DigitsOnTurbo: Optimizing Large Number Arithmetic Operations"><meta name=twitter:description content="This project is part of my M.Tech Thesis at IIT Gandhinagar, exploring optimizations for arithmetic operations on large numbers."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://iamsubhrajit10.me/posts/"},{"@type":"ListItem","position":2,"name":"DigitsOnTurbo: Optimizing Large Number Arithmetic Operations","item":"https://iamsubhrajit10.me/posts/digitsonturbo/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"DigitsOnTurbo: Optimizing Large Number Arithmetic Operations","name":"DigitsOnTurbo: Optimizing Large Number Arithmetic Operations","description":"This project is part of my M.Tech Thesis at IIT Gandhinagar, exploring optimizations for arithmetic operations on large numbers.","keywords":["DigitsOnTurbo","Large Number Arithmetic","Optimization","Vedic Mathematics","Addition","Subtraction","Multiplication","GNU Multiprecision Library"],"articleBody":"Working on this project with Prof. Abhishek Bichhawat (my supercool advisor, a blessing for me to have him) and Prof. Yuvraj Patel (my co-advisor, appreciate him for his observations that I most often overlook!).\nI started working on this project in January 2024. Although some undergrad students at IITGN began exploring it earlier, I essentially restarted from scratch. :)\nWe’re exploring arithmetic operations performed on large numbers, aiming to optimize them for better performance. By “large numbers,” we mean those that don’t fit into the traditional variable sizes of programming languages. For example, in cryptography, we often deal with numbers that are 2048, 4096, or even more bits in size. Traditional compilers don’t natively support such sizes, and as cryptographers push for more security, they continue to increase the bit-sizes. To handle these large numbers, programmers often write assembly code or use highly-optimized libraries like the GNU Multiprecision Arithmetic Library (GMP for short).\nOur objective has been to explore whether any further optimizations can be made to GMP’s approach or if a different approach could match GMP’s performance while possibly providing other benefits. For now, due to the constraints of my thesis timeline, we’ve focused on multiplication, addition, and subtraction operations. Initially, I felt a bit discouraged, as GMP has been around since the 1990s, and I wondered how I could match or surpass their performance within just a few months. But my advisors motivated me to dig deeper, and sure enough, there are some different approaches to multiplication, addition, and subtraction that, under certain conditions, can improve or at least match the performance of these operations. UPDATE: Currently subtraction is nearly 2x faster than GMP :D\nThe main objective of this project is to design and implement approaches/algorithms that can speed up arithmetic operations on large numbers. Our one further goal, from the beginning was to somehow infuse the strategies deployed by our Indian Vedic Mathematics into the existing algorithms.\nGit Repo: Large-Number-Arithmetic-Operations.\nProgress so far Here’s where things stand with the project right now. I’ll keep updating this as I make progress!\nGetting started with THP, PERF, RDTSC(P), RUSAGE Compiler Optimizations, GDB, VALGRIND For the first few months (approx. four, side by side with my coursework), I focused on getting the logistics right and spent time learning kernel modifications, how to use Transparent Huge Pages (THP), PERF, GDB, and Valgrind. For measurement of accurate CPU Cycles/Ticks, we also had to account for using RDTSC. Also, there’s one more API call provided by the kernel, getrusage, which provides resource usage statistics for the calling process.\nTransparent Huge Pages (THP) Huge pages are a game-changer, as they reduce TLB misses by using page sizes larger than the default 4 KiB. Linux transparent huge page (THP) support lets the kernel automatically promote regular memory pages into huge pages.\nYou can actually hint the kernel to allocate a huge page of size 2MB/1GB (depending on the system support) on the fly! I used madvise system calls directly from my C code. A critical point here: after allocating a THP, the kernel won’t treat it as a THP until you’ve initialized the memory segment. If you skip this step, the pages will be converted on the fly, which will hurt your code’s performance.\nSome helpful resources on Huge Pages and THP:\nUsing Huge Pages on Linux | Erik Rigtorp Kernel index Transparent Hugepage Support — The Linux Kernel documentation How to use, monitor, and disable transparent hugepages in Red Hat Enterprise Linux 6 and 7? - Red Hat Customer Portal Examining Huge Pages or Transparent Huge Pages performance | Red Hat Developer The basic idea of THP in this project: If your code requires a lot of memory (like our does), try pre-allocating a large chunk using THP. This can help avoid frequent page faults within the memory segment.\nYou can check out my code where I’ve used THP in this GitHub repo (just look for files with ‘THP’ in the name).\nChanging Page Size, System-Wide To get a page size larger than the traditional 4 KiB, I initially tried recompiling the kernel by modifying relevant code segments. But I ran into issues, especially with Intel’s architecture. On x86_64 processors (at least mine), it only supports a 4KB page size. Changing the page size through the config file didn’t work either. Even after attempting to modify the kernel source code (like grep’ing for ‘#define PAGE_’), make failed due to compiler assertions, and bypassing them didn’t work.\nThat said, based on architecture specs, some architectures like ARM support page sizes of 4 KiB, 16 KiB, and 64 KiB at a system-wide level. Apple also uses 16 KiB page sizes on its ARM-based machines. I’m yet to try those, and maybe in the future, I’ll give them a shot!\nPERF PERF is a powerful performance analysis tool in Linux, widely used for profiling applications and understanding performance metrics. It can capture a range of data points, such as CPU cycles, user/kernel instructions, page faults, cache misses, and more.\nIn our project, we’re leveraging PERF to gain deep insights into how our code behaves under different conditions. Instead of relying solely on the usual perf through shell, we’re also using the perf_event_open system call to measure performance at a more granular level. This allows us to target specific code fragments and gather precise performance data, offering more flexibility and control over the profiling process.\nRDTSC(P) To measure hardware ticks/CPU cycles accurately without much overhead (which PERF adds), we initially used RDTSC ticks. However, it is well-known that RDTSC does not provide accurate measurements in cases of code cross-contamination due to out-of-order execution. To address this, we explored proper benchmarking techniques using RDTSC and found a white paper by Intel that explains how to measure ticks accurately using a combination of CPUID, RDTSC, and RDTSCP instructions. You can find the white paper here: How to Benchmark Code Execution Times on Intel® IA-32 and IA-64 Instruction Set Architectures. We used this benchmarking methodology throughout our work for reporting relative ticks.\nRUSAGE In linux systems, getrusage is a system call that provides resource usage statistics for the calling process. It is used to measure the resources used by the process, like CPU time, memory usage, etc. We used this system call to measure the CPU time used by our code. POSIX.1 specifies getrusage(), but specifies only the fields ru_utime and ru_stime. And, for our benchmarking purposes, along with other tools: PERF, RDTSC(P), and Timespec (clock_gettime()) we also utilized ru_utime and ru_stime to measure the user CPU time and system CPU time, respectively.\nCompiler Optimizations Initially I thought that to write vectorized code, we needed to explicitly use SIMD intrinsics. However later I realized that the compiler can do a lot of optimizations for us. I was not aware that -O3 flag can do auto-vectorization itself (if certain conditions are satisfied). By default, it uses SSE, but you can specify the architecture to use AVX or AVX512, with proper flags. As per gcc docs, the O3 flag includes -fvect-cost-model=dynamic and -fversion-loops-for-strides, which enable the compiler to auto-vectorize the code, including multiple versions of the improve performance by optimizing loops for various memory access patterns.\nVectorization also depends upon the alignment of the data. If the data is not aligned properly, the compiler may not be able to vectorize the code. Intel documentation suggests the below for data alignment:\n16-byte alignment for SSE 32-byte alignment for AVX (and AVX2) 64-byte alignment for AVX512 It suggests to use _mm_malloc(ptr,) and _mm_free(ptr) for memory allocation and deallocation. Also, it suggests to use assume_aligned attribute to hint the compiler about the alignment of the data.\nIn a nut-shell, the white paper by Intel suggests to ensure that the data is aligned properly, and to use the assume_aligned attribute to hint the compiler about the alignment of the data. This will help the compiler to vectorize the code efficiently.\nWe can check if the code is vectorized by the GCC compiler by using the -ftree-vectorizer-verbose=2 flag. This flag provides detailed information about the vectorization process. Additionally, you can use the -fopt-info-vec flag to generate a report on vectorization.\nI analyzed a lot of permutations and combinations with the gcc compiler flags, and here are some of the observations:\nObservations: For code written without any manual vectorization, i.e. no use of SIMD intrinsics in the code:\nOnly -O3: (Case 1) Auto-vectorizes loops using SSE instructions, if possible. Does not generate AVX or AVX512 instructions. Without -O3, with -mavx512f: (Case 2) Does not auto-vectorize loops at all. -O3 -mavx512f: (Case 3) Auto-vectorizes the loops using AVX512 instructions, if possible. For code written with manual vectorization, i.e. using SIMD intrinsics in the code, basically I tried with AVX512 intrinsics. And to work with AVX512, I mandatorily need to use -mavx512f flag. And, I observed that:\nWith only -mavx512f: (Case 4) Does not auto-vectorize any loops. But, generates AVX512 instructions for the manual vectorized code, replacing the intrinsics with the corresponding AVX512 instructions. With -O3 -mavx512f: (Case 5) Auto-vectorizes all the loops using AVX512 instructions, except the ones that are already manually vectorized. Generates AVX512 instructions for the manual vectorized code, replacing the intrinsics with the corresponding AVX512 instructions. Below are some of the generated assembly code snippets for the above cases:\nFor the non-explicitly vectorized code, the assembly code generated is as follows:\nCase 1: [Only -O3] 1 2 3 4 5 6 7 8 9 10 11 12 .L580: # sub.c:204: result[i] = a[i] - b[i]; movdqu\t0(%r13,%rdx), %xmm2\t# MEM [(uint64_t *)a_156 + ivtmp.458_478 * 1], vect__163.334 movdqu\t(%r12,%rdx), %xmm9\t# MEM [(uint64_t *)b_159 + ivtmp.458_478 * 1], tmp1968 psubq\t%xmm9, %xmm2\t# tmp1968, vect__163.334 # sub.c:204: result[i] = a[i] - b[i]; movups\t%xmm2, (%r14,%rdx)\t# vect__163.334, MEM [(uint64_t *)result_133 + ivtmp.458_478 * 1] # sub.c:213: borrow_array[i] = 0; movups\t%xmm10, (%rcx,%rdx)\t# tmp928, MEM [(uint64_t *)borrow_array_152 + ivtmp.458_478 * 1] addq\t$16, %rdx\t#, ivtmp.458 cmpq\t%r8, %rdx\t# _485, ivtmp.458 je\t.L716\t#, Case 2: [Without -O3, with -mavx512f] 1 2 3 4 5 6 7 8 .L126: # sub.c:204: result[i] = a[i] - b[i]; movl\t-1584(%rbp), %eax\t# i, tmp360 cltq leaq\t0(,%rax,8), %rdx\t#, _224 movq\t-1320(%rbp), %rax\t# a, tmp361 addq\t%rdx, %rax\t# _224, _226 movq\t(%rax), %rdx\t# *_226, _227 Case 3: [-O3 -mavx512f] 1 2 3 4 5 6 7 8 9 .L599: # sub.c:204: result[i] = a[i] - b[i]; vmovdqu64\t(%r14,%rax), %zmm6\t# MEM [(uint64_t *)a_156 + ivtmp.561_842 * 1], tmp2081 vpsubq\t(%r12,%rax), %zmm6, %zmm11\t# MEM [(uint64_t *)b_159 + ivtmp.561_842 * 1], tmp2081, vect__163.380 # sub.c:204: result[i] = a[i] - b[i]; vmovdqu64\t%zmm11, (%r15,%rax)\t# vect__163.380, MEM [(uint64_t *)result_133 + ivtmp.561_842 * 1] # sub.c:213: borrow_array[i] = 0; vmovdqu64\t%zmm8, (%rcx,%rax)\t# tmp1164, MEM [(uint64_t *)borrow_array_152 + ivtmp.561_842 * 1] addq\t$64, %rax\t#, ivtmp.561 For the manual vectorized code, the assembly code generated is as follows:\nCase 4: [With only -mavx512f] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 # sub_avx_64_aligned.c:218: for (i = 0; i \u003c n; i += 8) movl\t$0, -3448(%rbp)\t#, i # sub_avx_64_aligned.c:218: for (i = 0; i \u003c n; i += 8) jmp\t.L137\t# .L145: # sub_avx_64_aligned.c:222: a_vec = _mm512_load_si512(a + i); movl\t-3448(%rbp), %eax\t# i, tmp394 cltq leaq\t0(,%rax,8), %rdx\t#, _242 # sub_avx_64_aligned.c:222: a_vec = _mm512_load_si512(a + i); movq\t-2720(%rbp), %rax\t# a, tmp395 addq\t%rdx, %rax\t# _242, _243 movq\t%rax, -2664(%rbp)\t# _243, __P # /usr/lib/gcc/x86_64-linux-gnu/11/include/avx512fintrin.h:542: return *(__m512i *) __P; movq\t-2664(%rbp), %rax\t# __P, tmp396 vmovdqa64\t(%rax), %zmm0\t# MEM[(__m512i * {ref-all})__P_244], _245 # sub_avx_64_aligned.c:222: a_vec = _mm512_load_si512(a + i); vmovdqa64\t%zmm0, -2544(%rbp)\t# _245, a_vec # sub_avx_64_aligned.c:223: b_vec = _mm512_load_si512(b + i); movl\t-3448(%rbp), %eax\t# i, tmp397 cltq leaq\t0(,%rax,8), %rdx\t#, _249 # sub_avx_64_aligned.c:223: b_vec = _mm512_load_si512(b + i); movq\t-2712(%rbp), %rax\t# b, tmp398 addq\t%rdx, %rax\t# _249, _250 movq\t%rax, -2656(%rbp)\t# _250, __P # /usr/lib/gcc/x86_64-linux-gnu/11/include/avx512fintrin.h:542: return *(__m512i *) __P; movq\t-2656(%rbp), %rax\t# __P, tmp399 vmovdqa64\t(%rax), %zmm0\t# MEM[(__m512i * {ref-all})__P_251], _252 # sub_avx_64_aligned.c:223: b_vec = _mm512_load_si512(b + i); vmovdqa64\t%zmm0, -2480(%rbp)\t# _252, b_vec vmovdqa64\t-2544(%rbp), %zmm0\t# a_vec, tmp400 vmovdqa64\t%zmm0, -2416(%rbp)\t# tmp400, __A vmovdqa64\t-2480(%rbp), %zmm0\t# b_vec, tmp401 vmovdqa64\t%zmm0, -2352(%rbp)\t# tmp401, __B # /usr/lib/gcc/x86_64-linux-gnu/11/include/avx512fintrin.h:762: return (__m512i) ((__v8du) __A - (__v8du) __B); vmovdqa64\t-2416(%rbp), %zmm0\t# __A, _257 # /usr/lib/gcc/x86_64-linux-gnu/11/include/avx512fintrin.h:762: return (__m512i) ((__v8du) __A - (__v8du) __B); vmovdqa64\t-2352(%rbp), %zmm1\t# __B, _258 # /usr/lib/gcc/x86_64-linux-gnu/11/include/avx512fintrin.h:762: return (__m512i) ((__v8du) __A - (__v8du) __B); vpsubq\t%zmm1, %zmm0, %zmm0\t# _258, _257, _259 # sub_avx_64_aligned.c:227: result_vec = _mm512_sub_epi64(a_vec, b_vec); vmovdqa64\t%zmm0, -2288(%rbp)\t# _260, result_vec # sub_avx_64_aligned.c:231: __mmask8 borrow_mask = _mm512_cmplt_epi64_mask(result_vec, zeros); vmovdqa64\tzeros(%rip), %zmm0\t# zeros, zeros.47_263 vmovdqa64\t-2288(%rbp), %zmm1\t# result_vec, tmp402 vmovdqa64\t%zmm1, -2224(%rbp)\t# tmp402, __X vmovdqa64\t%zmm0, -2160(%rbp)\t# zeros.47_263, __Y # /usr/lib/gcc/x86_64-linux-gnu/11/include/avx512fintrin.h:9896: return (__mmask8) __builtin_ia32_cmpq512_mask ((__v8di) __X, vmovdqa64\t-2224(%rbp), %zmm0\t# __X, tmp403 vmovdqa64\t-2160(%rbp), %zmm1\t# __Y, tmp404 movl\t$-1, %eax\t#, tmp405 kmovw\t%eax, %k1\t# tmp405, tmp405 vpcmpq\t$1, %zmm1, %zmm0, %k0{%k1}\t#, tmp404, tmp403, _266, tmp405 kmovw\t%k0, %eax\t# _266, _266 Case 5: [-O3 -mavx512f] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 .L348: # /usr/lib/gcc/x86_64-linux-gnu/11/include/avx512fintrin.h:762: return (__m512i) ((__v8du) __A - (__v8du) __B); vmovdqa64\t(%r12,%rax,8), %zmm0\t# MEM[(__m512i * {ref-all})a_179 + ivtmp.520_1112 * 8], tmp1805 vpsubq\t(%r10,%rax,8), %zmm0, %zmm3\t# MEM[(__m512i * {ref-all})b_182 + ivtmp.520_1112 * 8], tmp1805, tmp1277 # sub_avx_64_aligned.c:218: for (i = 0; i \u003c n; i += 8) leaq\t8(%rax), %rdx\t#, tmp1209 leaq\t16(%rax), %r8\t#, ivtmp.520 leaq\t24(%rax), %r11\t#, ivtmp.520 # /usr/lib/gcc/x86_64-linux-gnu/11/include/avx512fintrin.h:9896: return (__mmask8) __builtin_ia32_cmpq512_mask ((__v8di) __X, vpcmpq\t$1, zeros(%rip), %zmm3, %k5\t#, zeros, tmp1277, tmp1279 # /usr/lib/gcc/x86_64-linux-gnu/11/include/avx512fintrin.h:741: return (__m512i) __builtin_ia32_paddq512_mask ((__v8di) __A, vmovdqa64\t%zmm3, %zmm4\t# tmp1277, tmp1277 vpaddq\tlimb_digits(%rip), %zmm3, %zmm4{%k5}\t# limb_digits, tmp1277, tmp1277, tmp1279, tmp1277 # /usr/lib/gcc/x86_64-linux-gnu/11/include/avx512fintrin.h:4201: __builtin_ia32_pbroadcastq512_gpr_mask (__A, vpbroadcastq\t%r9, %zmm8{%k5}{z}\t# tmp1057, tmp1282, tmp1279, # /usr/lib/gcc/x86_64-linux-gnu/11/include/avx512fintrin.h:575: *(__m512i *) __P = __A; vmovdqa64\t%zmm8, (%rdi,%rax,8)\t# tmp1282, MEM[(__m512i * {ref-all})borrow_array_175 + ivtmp.520_1112 * 8] vmovdqa64\t%zmm4, (%rbx,%rax,8)\t# tmp1281, MEM[(__m512i * {ref-all})result_156 + ivtmp.520_1112 * 8] # /usr/lib/gcc/x86_64-linux-gnu/11/include/avx512fintrin.h:762: return (__m512i) ((__v8du) __A - (__v8du) __B); vmovdqa64\t(%r12,%rdx,8), %zmm9\t# MEM[(__m512i * {ref-all})a_179 + ivtmp.520_1112 * 8], tmp1807 # sub_avx_64_aligned.c:218: for (i = 0; i \u003c n; i += 8) addq\t$32, %rax\t#, ivtmp.520 # /usr/lib/gcc/x86_64-linux-gnu/11/include/avx512fintrin.h:762: return (__m512i) ((__v8du) __A - (__v8du) __B); vpsubq\t(%r10,%rdx,8), %zmm9, %zmm1\t# MEM[(__m512i * {ref-all})b_182 + ivtmp.520_1112 * 8], tmp1807, tmp1285 # /usr/lib/gcc/x86_64-linux-gnu/11/include/avx512fintrin.h:9896: return (__mmask8) __builtin_ia32_cmpq512_mask ((__v8di) __X, vpcmpq\t$1, zeros(%rip), %zmm1, %k6\t#, zeros, tmp1285, tmp1287 # /usr/lib/gcc/x86_64-linux-gnu/11/include/avx512fintrin.h:741: return (__m512i) __builtin_ia32_paddq512_mask ((__v8di) __A, vpaddq\tlimb_digits(%rip), %zmm1, %zmm1{%k6}\t# limb_digits, tmp1285, tmp1285, tmp1287, tmp1285 # /usr/lib/gcc/x86_64-linux-gnu/11/include/avx512fintrin.h:4201: __builtin_ia32_pbroadcastq512_gpr_mask (__A, vpbroadcastq\t%r9, %zmm5{%k6}{z}\t# tmp1057, tmp1290, tmp1287, # /usr/lib/gcc/x86_64-linux-gnu/11/include/avx512fintrin.h:575: *(__m512i *) __P = __A; vmovdqa64\t%zmm5, (%rdi,%rdx,8)\t# tmp1290, MEM[(__m512i * {ref-all})borrow_array_175 + ivtmp.520_1112 * 8] vmovdqa64\t%zmm1, (%rbx,%rdx,8)\t# tmp1289, MEM[(__m512i * {ref-all})result_156 + ivtmp.520_1112 * 8] # /usr/lib/gcc/x86_64-linux-gnu/11/include/avx512fintrin.h:762: return (__m512i) ((__v8du) __A - (__v8du) __B); vmovdqa64\t(%r12,%r8,8), %zmm11\t# MEM[(__m512i * {ref-all})a_179 + ivtmp.520_1112 * 8], tmp1809 vpsubq\t(%r10,%r8,8), %zmm11, %zmm12\t# MEM[(__m512i * {ref-all})b_182 + ivtmp.520_1112 * 8], tmp1809, tmp1293 # /usr/lib/gcc/x86_64-linux-gnu/11/include/avx512fintrin.h:9896: return (__mmask8) __builtin_ia32_cmpq512_mask ((__v8di) __X, vpcmpq\t$1, zeros(%rip), %zmm12, %k7\t#, zeros, tmp1293, tmp1295 # /usr/lib/gcc/x86_64-linux-gnu/11/include/avx512fintrin.h:741: return (__m512i) __builtin_ia32_paddq512_mask ((__v8di) __A, vmovdqa64\t%zmm12, %zmm10\t# tmp1293, tmp1293 vpaddq\tlimb_digits(%rip), %zmm12, %zmm10{%k7}\t# limb_digits, tmp1293, tmp1293, tmp1295, tmp1293 # /usr/lib/gcc/x86_64-linux-gnu/11/include/avx512fintrin.h:4201: __builtin_ia32_pbroadcastq512_gpr_mask (__A, vpbroadcastq\t%r9, %zmm13{%k7}{z}\t# tmp1057, tmp1298, tmp1295, # /usr/lib/gcc/x86_64-linux-gnu/11/include/avx512fintrin.h:575: *(__m512i *) __P = __A; vmovdqa64\t%zmm13, (%rdi,%r8,8)\t# tmp1298, MEM[(__m512i * {ref-all})borrow_array_175 + ivtmp.520_1112 * 8] vmovdqa64\t%zmm10, (%rbx,%r8,8)\t# tmp1297, MEM[(__m512i * {ref-all})result_156 + ivtmp.520_1112 * 8] # /usr/lib/gcc/x86_64-linux-gnu/11/include/avx512fintrin.h:762: return (__m512i) ((__v8du) __A - (__v8du) __B); vmovdqa64\t(%r12,%r11,8), %zmm14\t# MEM[(__m512i * {ref-all})a_179 + ivtmp.520_1112 * 8], tmp1811 vpsubq\t(%r10,%r11,8), %zmm14, %zmm1\t# MEM[(__m512i * {ref-all})b_182 + ivtmp.520_1112 * 8], tmp1811, tmp1301 # /usr/lib/gcc/x86_64-linux-gnu/11/include/avx512fintrin.h:9896: return (__mmask8) __builtin_ia32_cmpq512_mask ((__v8di) __X, vpcmpq\t$1, zeros(%rip), %zmm1, %k1\t#, zeros, tmp1301, tmp1303 # /usr/lib/gcc/x86_64-linux-gnu/11/include/avx512fintrin.h:741: return (__m512i) __builtin_ia32_paddq512_mask ((__v8di) __A, vpaddq\tlimb_digits(%rip), %zmm1, %zmm1{%k1}\t# limb_digits, tmp1301, tmp1301, tmp1303, tmp1301 # /usr/lib/gcc/x86_64-linux-gnu/11/include/avx512fintrin.h:4201: __builtin_ia32_pbroadcastq512_gpr_mask (__A, vpbroadcastq\t%r9, %zmm7{%k1}{z}\t# tmp1057, tmp1306, tmp1303, # /usr/lib/gcc/x86_64-linux-gnu/11/include/avx512fintrin.h:575: *(__m512i *) __P = __A; vmovdqa64\t%zmm7, (%rdi,%r11,8)\t# tmp1306, MEM[(__m512i * {ref-all})borrow_array_175 + ivtmp.520_1112 * 8] vmovdqa64\t%zmm1, (%rbx,%r11,8)\t# tmp1305, MEM[(__m512i * {ref-all})result_156 + ivtmp.520_1112 * 8] # sub_avx_64_aligned.c:218: for (i = 0; i \u003c n; i += 8) cmpl\t%eax, %r14d\t# ivtmp.520, n_limb.37_27 jg\t.L348\t#, GDB GDB is the GNU Debugger, a widely-used tool that lets you see what’s happening inside a program while it runs or what it was doing when it crashed. We’ve relied heavily on GDB to debug various issues, trace the program flow of the algorithms we’ve implemented, and analyze the assembly code generated after compiler optimizations.\nValgrind Valgrind is a powerful tool for memory debugging, leak detection, and profiling. It has been indispensable in our project for identifying memory leaks and ensuring that our program uses memory efficiently, helping us maintain stability and performance throughout the development process.\nTiming Measurements To correctly measure the performance of our algorithms, we need to ensure that our timing mechanisms are accurate and reliable. We’ve explored various timing methods to benchmark our code effectively and resorted to using the below technique:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 #define TIME_(t, func) \\ do \\ { \\ unsigned long long __t0, __t1, __times, __tmp; \\ __times = 1; \\ { \\ func; \\ } \\ do \\ { \\ __times \u003c\u003c= 1; \\ __t0 = measure_start(); \\ for (int __t = 0; __t \u003c __times; __t++) \\ { \\ func; \\ } \\ __t1 = measure__end(); \\ __tmp = __t1 - __t0; \\ } while (__tmp \u003c THRESHOLD); \\ (t) = (double)(__tmp) / __times; \\ } while (0) where, can be RDTSC, RUSAGE, TIMESPEC etc. and measure_start() and measure_end() are the functions to measure the start and end times, respectively. This macro repeatedly doubles the number of iterations until the measured time exceeds a predefined threshold, ensuring that the timing is accurate and stable. And, returns the average time taken as per the last executed loopin which the threshold is crossed.\nVedic Mathematical Algorithms God Resource: Vedic Mathematics by Bharati Krishna Tirth Ji Maharaj\nOnce we got comfortable with the tools, we shifted focus to tweaking Vedic mathematical algorithms like Urdhva Tiryagbhyam, particularly for multiplication, described in the later part of the post.\nAddition Optimizing addition can be challenging due to the direct dependency of carries in subsequent digit additions when adding two numbers. This dependency may create performance bottlenecks, as each digit addition must wait for the previous carry to be generated before proceeding. Consequently, many libraries do not leverage parallel computation in their addition implementations for large numbers. However, in certain scenarios, the addition of partial digits may not propagate carries to subsequent digit additions. By exploiting this behavior, we might design our addition algorithm such that the additions of digits become independent of each other. This independence could enable us to process the digit additions in parallel, potentially yielding significant performance improvements.\nThe key to optimizing addition operations is to partition the digits in a way that minimizes the likelihood of carry propagation in a chained manner. We will approach this by implementing a two-phase process for adding large numbers:\nFirst Phase: Perform the independent, simultaneous addition of the two digits without initially accounting for carries. This phase aims to complete the digit-wise additions as much as possible in parallel, reducing the immediate dependency on carry propagation.\nSecond Phase: After the initial addition is complete, identify and account for any carries generated. These carries will then be added to the partial sum result from the first phase.\nIt is important to note that while the second phase may introduce additional carries that require propagation, our goal is to design the digit partitioning to minimize the probability of such scenarios. In cases where multiple rounds of carry propagation are necessary, we will handle them efficiently to avoid significant performance degradation. This approach aims to leverage advanced processing techniques like AVX or multithreading, thus improving overall performance while managing the complexity introduced by carries.\nGrouping of Digits for Efficient Processing for Addition When performing arithmetic operations on typical computers, the Instruction Set Architecture (ISA) is designed to process data in 32-bit or 64-bit chunks. To efficiently utilize these data processing units, we can group multiple digits together into a single unit. Specifically, with a 32-bit unit, the maximum value that can be represented in an unsigned format is 4,294,967,295. By grouping nine consecutive digits into a 32-bit data unit, we can safely accommodate potential partial sum overflows during addition operations. This approach reduces the number of arithmetic operations required, as more digits are processed in each operation, thereby improving computational performance.\nExample of Digit Grouping for Addition Assume we are grouping nine digits, and let the two numbers x and y be as follows:\nx = 125634987654321987654321987\ny = 917683012345678012345678012.\nSince we are grouping nine digits internally,\nx will be stored as \u003c125634987, 654321987, 654321987\u003e,\nand y will be stored as \u003c917683012, 345678012, 345678012\u003e.\nAdditionally, if the number of digits is not a multiple of nine, we will prepend zeros to the numbers.\nAvoiding Chained Propagation of Carries As we have grouped a certain number of digits together and treated them as a single unit, packing them into a data structure of 32 or 64 bits, we have reduced the likelihood of encountering scenarios that necessitate extensive carry propagation. This grouping approach minimizes the chance of cascading carry dependencies across the entire number and enables more efficient processing.\nFor example, if we group digits into 32-bit or 64-bit blocks, the carry propagation within each block is localized, thereby reducing the impact on overall performance. This method allows for parallel processing of multiple blocks, leveraging advanced technologies such as AVX or multithreading. As a result, the algorithm can handle larger numbers more efficiently and maintain high performance despite the inherent complexity of carry management.\nExamples of Chained Carry Propagation Consider the following example:\nExample 1: a = 999...9999\nb = 000...0001\nAdding these two numbers will generate a carry for each digit addition, resulting in a chained carry propagation from the least significant digit to the most significant digit. For instance, if we group k = 2 digits together, where:\na' = 99 99 ... 99 99 99 b' = 00 00 ... 00 00 01\nThe addition in the first phase without accounting for carries might yield:\nPartial Sum = 0, 0, 0, ..., 0, 0, 100\nHere, the carries generated would be:\nCarries = 0, 0, 0, ..., 0, 1, 0\nTo obtain the correct result, we need to propagate the carry in a chained manner and adjust the partial sum accordingly. This chained propagation involves processing carries sequentially, which can degrade performance if not handled efficiently.\nQuantifying the Likelihood of Chained Carry Propagation in Addition To understand how frequently such problematic scenarios occur, we can analyze the probability of encountering such cases mathematically.\nProbability Analysis:\nAssume we are working with n-digit numbers, and we group k digits together into a single 32-bit or 64-bit block. The probability of chained carry propagation depends on the likelihood that the partial sums from the first phase will generate additional carries when processed in the second phase. An analysis of how likely those cases will occur will be posted here shortly!\nAlgorithm of Addition Implementation For our approach, Algorithm 1 illustrates the process of adding two large numbers represented as arrays of digits, where each index packs nine digits together. We use 32-bit storage for each index, and while it can technically hold up to 10 digits, we’ve limited it to nine digits to account for overflow.\nThe algorithm operates in two distinct phases:\nFirst Phase (Addition without Carry Propagation): In this phase, we add the grouped digits from both arrays without waiting for the carry to be propagated. If the resulting sum at any position exceeds nine digits (i.e., 10^9 or more), we generate a carry for that position and subtract 10^9 from the sum. This adjustment ensures that the sum remains within nine digits. The reason for setting the carry array and adjusting the sum in this way is that adding two nine-digit numbers will never result in more than 10 digits, so any overflow is correctly handled by setting the carry. Carry Array Left Shift: Before proceeding to the second phase, the carry array is left-shifted by one position. This shift is necessary because the carry generated in the current partial sum needs to be propagated to the subsequent partial sums in the next phase. Second Phase (Carry Propagation): In this phase, we add the carries generated during the first phase to the corresponding partial sums. This ensures that the final result accounts for all necessary carries and produces the correct sum. Note: Since each index stores nine digits, the likelihood of generating additional carries during the second phase is relatively low, as discussed earlier. However, it is crucial to have a mechanism in place to check and handle any remaining carry propagation to ensure the correctness of the algorithm.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 Algorithm 1: GROUPED_ADD Preprocessing: a, b → unsigned 32-bit integer arrays, each index packed with 9 digits, of length n. 1st Phase: Perform additions, without halting for carry generation: for i = 0 to n: sum[i] ← a[i] + b[i] if sum[i] ≥ 10^9: carry_array[i] ← 1 sum[i] ← sum[i] - 10^9 LeftShift(carry_array, 1): Fill LSB with 0 2nd Phase: Perform the carry propagation: for i = 0 to n: sum[i] ← sum[i] + carry_array[i] Check if carry still persists, and if so, handle carry propagation normally. Return the array sum. Now that we understand the basic mechanism of our addition algorithm, we can observe that each addition within both the two phases is independent of each other; it is just that phase two needs to wait until phase one has been completed. Using this observation, we can simultaneously process the addition within each phase, leveraging multiple operations processed at once. Therefore, SIMD (Single Instruction, Multiple Data) can be leveraged for data parallelism, while multi-threading can be used to implement task parallelism. Additionally, other techniques, such as GPU acceleration, may be applied to further exploit parallelism in the process of the additions.\nAlgorithm 2, below, illustrates how SIMD (Single Instruction, Multiple Data) can be utilized to perform simultaneous additions of grouped digits. By using 32-bit storage for each group of digits and leveraging 512-bit SIMD registers, we can pack up to 16 groups of digits into a single vector register. This configuration enables us to perform arithmetic operations on 16 groups of digits in parallel.\nSpecifically, since each group contains nine digits, the SIMD register can handle arithmetic operations on 16*9 = 144 digits simultaneously. This capability significantly reduces the overall execution time of the addition operation by processing multiple groups of digits in parallel.\nThe use of SIMD allows us to exploit parallelism to accelerate computations, which is particularly beneficial for efficiently handling large numbers. This approach enhances performance by minimizing the number of iterations needed to process all the digits.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 Algorithm 2: SIMD_GROUPED_ADD Preprocessing: a, b → unsigned 32-bit integer arrays, each index packed with 9 digits group, of length n. Assumption: a_vec, b_vec, c_vec, sum_vec are vector registers of size 512 bits, to be processed by SIMD instructions. 1st Phase: Perform additions, without halting for carry generation: for i = 0 to n increment by 16: a_vec ← Accumulate 16 elements from (a + i) b_vec ← Accumulate 16 elements from (b + i) sum_vec ← a_vec + b_vec for j = 0 to 16: if sum_vec[j] ≥ 10^9: carry_array[i + j] ← 1 sum_vec[j] ← sum_vec[j] - 10^9 LeftShift(carry_array, 1): Fill LSB with 0 2nd Phase: Perform the carry propagation: for i = 0 to n increment by 16: sum_vec ← sum_vec + carry_array If carry still persists, handle normally Return the array sum. Overall, these algorithms are designed to minimize the effects of carry propagation by grouping digits in a manner that reduces the probability of cascading carry dependencies. By handling independent operations in parallel, we can achieve significant performance improvements when adding large numbers, making these approaches well-suited for high-performance computing environments.\nPerformance Bottleneck of Addition When chained carry propagation occurs, carries generated from the one end of partial sums must be propagated to the other end of partial sums. This process can significantly degrade overall performance and impact parallelization, as it may require sequentially waiting for the carry to be generated for preceding partial sums. This introduces a significant dependency that can halt the parallel processing, thereby affecting performance. In our implementations, we’ve stuck to two phases of parallel processing of arithmetic operations, followed by a handle that checks for if further carry propagation is needed; if that is the case, we handle those cases simply by conventional sequential method of starting from the last group of partial sums to the most significant end. This hampers the overall performance of the implementation, but it ensures correctness in those scenarios.\nAnother alternative is to speculate or precompute the carries. However, it is important to analyze whether the cost of carry speculation justifies its benefits, as chained carry propagation typically affects only a limited number of cases.\nDespite these potential drawbacks, our approach speeds up the addition process overall. Since cases requiring extensive carry propagation are relatively rare, the benefits of enhanced performance in most scenarios outweigh the occasional performance hit from sequential handling.\nImplementation of Addition The implementation of Algorithm 1 is available here: github-group-add. The implementation of Algorithm 2 can be found here github-simd-add. Both implementations are written in C, with AVX512 intrinsics used for SIMD operations on Intel architectures.\nPlease note that these implementations may require modifications and will be updated periodically. Further details about the implementation will be provided soon!\nGMP’s mpz_add To be posted here shortly!\nGMP’s mpz_add Function Disassembled To be posted here shortly!\nExplicitly Vectorized Addition Disassembled To be posted here shortly!\nPerformance Comparison of Addition with GMP To be posted here shortly!\nSubtraction Details about the subtraction implementation will be posted here shortly!\nAlgorithm of Subtraction Implementation To be posted here shortly!\nImplementation of Subtraction using SIMD You can find the implementation of the subtraction algorithm here: github-explicit-vectorized. The implementation is written in C, with explicit vectorization using AVX512 intrinsics for Intel architectures.\nGMP’s mpz_sub GMP’s mpz_sub Function Disassembled Explicitly Vectorized Subtraction Disassembled Performance Comparison of Subtraction with GMP Below are some of the results of our explicitly vectorized subtraction implementation for Intel Skylake architecture, comparing the performance of the manual vectorized code with the GMP’s mpz_sub function. The results are presented in terms of the number of operations per second.\nRDTSC Random AVX vs GMP - Subtraction Timespec Random AVX vs GMP - Subtraction RUSAGE (ru_utime) Random AVX vs GMP - Subtraction Multiplication To be posted shortly!\nGrade-school Multiplication To be posted shortly!\nUrdhva-Tiryagbhyam We chose Urdhva-Tiryagbhyam due to its efficient handling of digits, which can be particularly beneficial for large number multiplication. It also offers caching benefits that align with our performance goals.\nKaratsuba To be posted shortly!\nToom-Cook To be posted shortly!\nFFT To be posted shortly!\n","wordCount":"5371","inLanguage":"en","image":"https://media.licdn.com/dms/image/v2/D4D03AQE-pgjjAFUfvQ/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1724069567965?e=1729728000\u0026v=beta\u0026t=WN9GYnbdp5lxYeVoj63bsjhJTf746I2lkprVOvY7cAA","datePublished":"2024-08-15T13:30:59+05:30","dateModified":"2024-09-23T13:30:59+05:30","author":{"@type":"Person","name":"Subhrajit"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://iamsubhrajit10.me/posts/digitsonturbo/"},"publisher":{"@type":"Organization","name":"Subhrajit Das","logo":{"@type":"ImageObject","url":"https://iamsubhrajit10.me/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://iamsubhrajit10.me accesskey=h title="Home (Alt + H)">Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://iamsubhrajit10.me/education/ title=Education><span>Education</span></a></li><li><a href=https://iamsubhrajit10.me/projects/ title=Projects><span>Projects</span></a></li><li><a href=https://iamsubhrajit10.me/publications/ title=Publications><span>Publications</span></a></li><li><a href=https://iamsubhrajit10.me/awards-achievements/ title="Awards & Achievements"><span>Awards & Achievements</span></a></li><li><a href=https://iamsubhrajit10.me/por/ title="Positions of Responsibility"><span>Positions of Responsibility</span></a></li><li><a href=https://iamsubhrajit10.me/research-interests/ title="Research Interests"><span>Research Interests</span></a></li><li><a href=https://iamsubhrajit10.me/cv/cv_subhrajit.pdf title=CV><span>CV</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://iamsubhrajit10.me>Home</a>&nbsp;»&nbsp;<a href=https://iamsubhrajit10.me/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">DigitsOnTurbo: Optimizing Large Number Arithmetic Operations</h1><div class=post-description>This project is part of my M.Tech Thesis at IIT Gandhinagar, exploring optimizations for arithmetic operations on large numbers.</div><div class=post-meta><span title='2024-08-15 13:30:59 +0530 IST'>August 15, 2024</span>&nbsp;·&nbsp;<span title='2024-09-23 13:30:59 +0530 IST'>Last updated on: September 23, 2024</span>&nbsp;·&nbsp;26 min&nbsp;·&nbsp;5371 words&nbsp;·&nbsp;Subhrajit&nbsp;|&nbsp;<a href=mailto:subhrajit.das@iitgn.ac.in rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#getting-started-with-thp-perf-rdtscp-rusage-compiler-optimizations-gdb-valgrind>Getting started with THP, PERF, RDTSC(P), RUSAGE Compiler Optimizations, GDB, VALGRIND</a><ul><li><a href=#transparent-huge-pages-thp>Transparent Huge Pages (THP)</a></li><li><a href=#changing-page-size-system-wide>Changing Page Size, System-Wide</a></li><li><a href=#perf>PERF</a></li><li><a href=#rdtscp>RDTSC(P)</a></li><li><a href=#rusage>RUSAGE</a></li><li><a href=#compiler-optimizations>Compiler Optimizations</a></li><li><a href=#gdb>GDB</a></li><li><a href=#valgrind>Valgrind</a></li></ul></li><li><a href=#timing-measurements>Timing Measurements</a></li><li><a href=#vedic-mathematical-algorithms>Vedic Mathematical Algorithms</a></li><li><a href=#addition>Addition</a><ul><li><a href=#grouping-of-digits-for-efficient-processing-for-addition>Grouping of Digits for Efficient Processing for Addition</a></li><li><a href=#avoiding-chained-propagation-of-carries>Avoiding Chained Propagation of Carries</a></li><li><a href=#quantifying-the-likelihood-of-chained-carry-propagation-in-addition>Quantifying the Likelihood of Chained Carry Propagation in Addition</a></li><li><a href=#algorithm-of-addition-implementation>Algorithm of Addition Implementation</a></li><li><a href=#performance-bottleneck-of-addition>Performance Bottleneck of Addition</a></li><li><a href=#implementation-of-addition>Implementation of Addition</a></li><li><a href=#gmps-mpz_add>GMP&rsquo;s mpz_add</a></li><li><a href=#performance-comparison-of-addition-with-gmp>Performance Comparison of Addition with GMP</a></li></ul></li><li><a href=#subtraction>Subtraction</a><ul><li><a href=#algorithm-of-subtraction-implementation>Algorithm of Subtraction Implementation</a></li><li><a href=#implementation-of-subtraction-using-simd>Implementation of Subtraction using SIMD</a></li><li><a href=#gmps-mpz_sub>GMP&rsquo;s mpz_sub</a></li><li><a href=#performance-comparison-of-subtraction-with-gmp>Performance Comparison of Subtraction with GMP</a></li></ul></li><li><a href=#multiplication>Multiplication</a><ul><li><a href=#grade-school-multiplication>Grade-school Multiplication</a></li><li><a href=#urdhva-tiryagbhyam>Urdhva-Tiryagbhyam</a></li><li><a href=#karatsuba>Karatsuba</a></li><li><a href=#toom-cook>Toom-Cook</a></li><li><a href=#fft>FFT</a></li></ul></li></ul></nav></div></details></div><div class=post-content><p>Working on this project with Prof. <a href=https://abhishek.people.iitgn.ac.in/>Abhishek Bichhawat</a> (my supercool advisor, a blessing for me to have him) and Prof. <a href=https://homepages.inf.ed.ac.uk/ypatel/>Yuvraj Patel</a> (my co-advisor, appreciate him for his observations that I most often overlook!).</p><p>I started working on this project in January 2024. Although some undergrad students at IITGN began exploring it earlier, I essentially restarted from scratch. :)</p><p>We’re exploring arithmetic operations performed on large numbers, aiming to optimize them for better performance. By &ldquo;large numbers,&rdquo; we mean those that don&rsquo;t fit into the traditional variable sizes of programming languages. For example, in cryptography, we often deal with numbers that are 2048, 4096, or even more bits in size. Traditional compilers don’t natively support such sizes, and as cryptographers push for more security, they continue to increase the bit-sizes. To handle these large numbers, programmers often write assembly code or use highly-optimized libraries like the <a href=https://gmplib.org/>GNU Multiprecision Arithmetic Library</a> (GMP for short).</p><p>Our objective has been to explore whether any further optimizations can be made to GMP&rsquo;s approach or if a different approach could match GMP&rsquo;s performance while possibly providing other benefits. For now, due to the constraints of my thesis timeline, we’ve focused on multiplication, addition, and subtraction operations. Initially, I felt a bit discouraged, as GMP has been around since the 1990s, and I wondered how I could match or surpass their performance within just a few months. But my advisors motivated me to dig deeper, and sure enough, there are some different approaches to multiplication, addition, and subtraction that, under certain conditions, can improve or at least match the performance of these operations. UPDATE: Currently subtraction is nearly 2x faster than GMP :D</p><p>The main objective of this project is to design and implement approaches/algorithms that can speed up arithmetic operations on large numbers. Our one further goal, from the beginning was to somehow infuse the strategies deployed by our Indian Vedic Mathematics into the existing algorithms.</p><p>Git Repo: <a href=https://github.com/iamsubhrajit10/Large-Number-Arithmetic-Operations>Large-Number-Arithmetic-Operations</a>.</p><h1 id=progress-so-far>Progress so far<a hidden class=anchor aria-hidden=true href=#progress-so-far>#</a></h1><p>Here’s where things stand with the project right now. I’ll keep updating this as I make progress!</p><h2 id=getting-started-with-thp-perf-rdtscp-rusage-compiler-optimizations-gdb-valgrind>Getting started with THP, PERF, RDTSC(P), RUSAGE Compiler Optimizations, GDB, VALGRIND<a hidden class=anchor aria-hidden=true href=#getting-started-with-thp-perf-rdtscp-rusage-compiler-optimizations-gdb-valgrind>#</a></h2><p>For the first few months (approx. four, side by side with my coursework), I focused on getting the logistics right and spent time learning kernel modifications, how to use Transparent Huge Pages (THP), PERF, GDB, and Valgrind.
For measurement of accurate CPU Cycles/Ticks, we also had to account for using RDTSC. Also, there&rsquo;s one more API call provided by the kernel, <code>getrusage</code>, which provides resource usage statistics for the calling process.</p><h3 id=transparent-huge-pages-thp>Transparent Huge Pages (THP)<a hidden class=anchor aria-hidden=true href=#transparent-huge-pages-thp>#</a></h3><p>Huge pages are a game-changer, as they reduce TLB misses by using page sizes larger than the default 4 KiB. Linux <a href=https://www.kernel.org/doc/html/latest/admin-guide/mm/transhuge.html>transparent huge page</a> (THP) support lets the kernel automatically promote regular memory pages into huge pages.</p><p>You can actually hint the kernel to allocate a huge page of size 2MB/1GB (depending on the system support) on the fly! I used <code>madvise</code> system calls directly from my C code. A critical point here: after allocating a THP, <strong>the kernel won’t treat it as a THP until you’ve initialized the memory segment. If you skip this step, the pages will be converted on the fly, which will hurt your code’s performance</strong>.</p><p>Some helpful resources on Huge Pages and THP:</p><ul><li><a href=https://rigtorp.se/hugepages/>Using Huge Pages on Linux | Erik Rigtorp</a></li><li><a href=https://lwn.net/Kernel/Index/#Huge_pages>Kernel index</a></li><li><a href=https://www.kernel.org/doc/html/latest/admin-guide/mm/transhuge.html#thp-sysfs>Transparent Hugepage Support — The Linux Kernel documentation</a></li><li><a href=https://access.redhat.com/solutions/46111>How to use, monitor, and disable transparent hugepages in Red Hat Enterprise Linux 6 and 7? - Red Hat Customer Portal</a></li><li><a href=https://developers.redhat.com/blog/2014/03/10/examining-huge-pages-or-transparent-huge-pages-performance#determining_whether_page_fault_latency_is_due_to_huge_pages_use>Examining Huge Pages or Transparent Huge Pages performance | Red Hat Developer</a></li></ul><p>The basic idea of THP in this project: If your code requires a lot of memory (like our does), try pre-allocating a large chunk using THP. This can help avoid frequent page faults within the memory segment.</p><p>You can check out my code where I’ve used THP in this <a href=https://github.com/iamsubhrajit10/Large-Number-Arithmetic-Operations>GitHub repo</a> (just look for files with &lsquo;THP&rsquo; in the name).</p><h3 id=changing-page-size-system-wide>Changing Page Size, System-Wide<a hidden class=anchor aria-hidden=true href=#changing-page-size-system-wide>#</a></h3><p>To get a page size larger than the traditional 4 KiB, I initially tried recompiling the kernel by modifying relevant code segments. But I ran into issues, especially with Intel’s architecture. On x86_64 processors (at least mine), it only supports a 4KB page size. Changing the page size through the config file didn’t work either. Even after attempting to modify the kernel source code (like grep&rsquo;ing for ‘#define PAGE_’), <code>make</code> failed due to compiler assertions, and bypassing them didn’t work.</p><p>That said, based on architecture specs, some architectures like ARM support page sizes of 4 KiB, 16 KiB, and 64 KiB at a system-wide level. Apple also uses 16 KiB page sizes on its ARM-based machines. I&rsquo;m yet to try those, and maybe in the future, I&rsquo;ll give them a shot!</p><h3 id=perf>PERF<a hidden class=anchor aria-hidden=true href=#perf>#</a></h3><p><a href=https://perf.wiki.kernel.org/index.php/Main_Page>PERF</a> is a powerful performance analysis tool in Linux, widely used for profiling applications and understanding performance metrics. It can capture a range of data points, such as CPU cycles, user/kernel instructions, page faults, cache misses, and more.</p><p>In our project, we&rsquo;re leveraging PERF to gain deep insights into how our code behaves under different conditions. Instead of relying solely on the usual <code>perf</code> through shell, we&rsquo;re also using the <a href=https://www.man7.org/linux/man-pages/man2/perf_event_open.2.html>perf_event_open</a> system call to measure performance at a more granular level. This allows us to target specific code fragments and gather precise performance data, offering more flexibility and control over the profiling process.</p><h3 id=rdtscp>RDTSC(P)<a hidden class=anchor aria-hidden=true href=#rdtscp>#</a></h3><p>To measure hardware ticks/CPU cycles accurately without much overhead (which PERF adds), we initially used RDTSC ticks. However, it is well-known that RDTSC does not provide accurate measurements in cases of code cross-contamination due to out-of-order execution. To address this, we explored proper benchmarking techniques using RDTSC and found a white paper by Intel that explains how to measure ticks accurately using a combination of CPUID, RDTSC, and RDTSCP instructions. You can find the white paper here: <a href="https://www.bing.com/ck/a?!&&amp;p=54968cdaea227f70JmltdHM9MTcyNDgwMzIwMCZpZ3VpZD0wMDg0M2FkMS02OTk3LTY2ZjctM2M1Ni0yZWZmNjg5MTY3YjMmaW5zaWQ9NTIyNA&amp;ptn=3&amp;ver=2&amp;hsh=3&amp;fclid=00843ad1-6997-66f7-3c56-2eff689167b3&amp;psq=How+to+Benchmark+Code+Execution+Times+on+Intel%c2%ae+IA-32+and+IA-64+Instruction+Set+Architectures&amp;u=a1aHR0cHM6Ly9jaXMudGVtcGxlLmVkdS9-cXplbmcvY2lzMzIwNy1zcHJpbmcxOC9maWxlcy9pYS0zMi1pYS02NC1iZW5jaG1hcmstY29kZS1leGVjdXRpb24tcGFwZXIucGRm&amp;ntb=1">How to Benchmark Code Execution Times on Intel® IA-32 and IA-64 Instruction Set Architectures</a>. We used this benchmarking methodology throughout our work for reporting relative ticks.</p><h3 id=rusage>RUSAGE<a hidden class=anchor aria-hidden=true href=#rusage>#</a></h3><p>In linux systems, <a href=https://www.man7.org/linux/man-pages/man2/getrusage.2.html><code>getrusage</code></a> is a system call that provides resource usage statistics for the calling process. It is used to measure the resources used by the process, like CPU time, memory usage, etc. We used this system call to measure the CPU time used by our code. POSIX.1 specifies getrusage(), but specifies only the fields <code>ru_utime</code> and <code>ru_stime</code>. And, for our benchmarking purposes, along with other tools: PERF, RDTSC(P), and Timespec (clock_gettime()) we also utilized <code>ru_utime</code> and <code>ru_stime</code> to measure the user CPU time and system CPU time, respectively.</p><h3 id=compiler-optimizations>Compiler Optimizations<a hidden class=anchor aria-hidden=true href=#compiler-optimizations>#</a></h3><p>Initially I thought that to write vectorized code, we needed to explicitly use SIMD intrinsics. However later I realized that the compiler can do a lot of optimizations for us. I was not aware that <code>-O3</code> flag can do auto-vectorization itself (if certain conditions are satisfied). By default, it uses SSE, but you can specify the architecture to use AVX or AVX512, with proper flags.<br>As per <a href=https://gcc.gnu.org/onlinedocs/gcc/Optimize-Options.html>gcc docs</a>, the <code>O3</code> flag includes <code>-fvect-cost-model=dynamic</code> and <code>-fversion-loops-for-strides</code>, which enable the compiler to auto-vectorize the code, including multiple versions of the improve performance by optimizing loops for various memory access patterns.</p><p>Vectorization also depends upon the alignment of the data. If the data is not aligned properly, the compiler may not be able to vectorize the code. <a href=https://www.intel.com/content/www/us/en/developer/articles/training/explicit-vector-programming-best-known-methods.html>Intel documentation</a> suggests the below for data alignment:</p><ul><li>16-byte alignment for SSE</li><li>32-byte alignment for AVX (and AVX2)</li><li>64-byte alignment for AVX512</li></ul><p>It suggests to use <code>_mm_malloc(ptr,&lt;alignment-size>)</code> and <code>_mm_free(ptr)</code> for memory allocation and deallocation. Also, it suggests to use assume_aligned attribute to hint the compiler about the alignment of the data.<br>In a nut-shell, the white paper by Intel suggests to ensure that the data is aligned properly, and to use the <code>assume_aligned</code> attribute to hint the compiler about the alignment of the data. This will help the compiler to vectorize the code efficiently.<br>We can check if the code is vectorized by the GCC compiler by using the <code>-ftree-vectorizer-verbose=2</code> flag. This flag provides detailed information about the vectorization process. Additionally, you can use the <code>-fopt-info-vec</code> flag to generate a report on vectorization.</p><p>I analyzed a lot of permutations and combinations with the gcc compiler flags, and here are some of the observations:</p><h4 id=observations>Observations:<a hidden class=anchor aria-hidden=true href=#observations>#</a></h4><p>For code written without any manual vectorization, i.e. no use of SIMD intrinsics in the code:</p><ul><li>Only <code>-O3</code>: (Case 1)<ul><li>Auto-vectorizes loops using SSE instructions, if possible.</li><li>Does not generate AVX or AVX512 instructions.</li></ul></li><li>Without <code>-O3</code>, with <code>-mavx512f</code>: (Case 2)<ul><li>Does not auto-vectorize loops at all.</li></ul></li><li><code>-O3 -mavx512f</code>: (Case 3)<ul><li>Auto-vectorizes the loops using AVX512 instructions, if possible.</li></ul></li></ul><p>For code written with manual vectorization, i.e. using SIMD intrinsics in the code, basically I tried with AVX512 intrinsics. And to work with AVX512, I mandatorily need to use <code>-mavx512f</code> flag. And, I observed that:</p><ul><li>With only <code>-mavx512f</code>: (Case 4)<ul><li>Does not auto-vectorize any loops.</li><li>But, generates AVX512 instructions for the manual vectorized code, replacing the intrinsics with the corresponding AVX512 instructions.</li></ul></li><li>With <code>-O3 -mavx512f</code>: (Case 5)<ul><li>Auto-vectorizes all the loops using AVX512 instructions, except the ones that are already manually vectorized.</li><li>Generates AVX512 instructions for the manual vectorized code, replacing the intrinsics with the corresponding AVX512 instructions.</li></ul></li></ul><p>Below are some of the generated assembly code snippets for the above cases:<br>For the non-explicitly vectorized code, the assembly code generated is as follows:</p><ul><li>Case 1: [Only <code>-O3</code>]<div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-0-1><a class=lnlinks href=#hl-0-1> 1</a>
</span><span class=lnt id=hl-0-2><a class=lnlinks href=#hl-0-2> 2</a>
</span><span class=lnt id=hl-0-3><a class=lnlinks href=#hl-0-3> 3</a>
</span><span class=lnt id=hl-0-4><a class=lnlinks href=#hl-0-4> 4</a>
</span><span class=lnt id=hl-0-5><a class=lnlinks href=#hl-0-5> 5</a>
</span><span class=lnt id=hl-0-6><a class=lnlinks href=#hl-0-6> 6</a>
</span><span class=lnt id=hl-0-7><a class=lnlinks href=#hl-0-7> 7</a>
</span><span class=lnt id=hl-0-8><a class=lnlinks href=#hl-0-8> 8</a>
</span><span class=lnt id=hl-0-9><a class=lnlinks href=#hl-0-9> 9</a>
</span><span class=lnt id=hl-0-10><a class=lnlinks href=#hl-0-10>10</a>
</span><span class=lnt id=hl-0-11><a class=lnlinks href=#hl-0-11>11</a>
</span><span class=lnt id=hl-0-12><a class=lnlinks href=#hl-0-12>12</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>  .L580:
</span></span><span class=line><span class=cl># sub.c:204:         result[i] = a[i] - b[i];
</span></span><span class=line><span class=cl>  movdqu	0(%r13,%rdx), %xmm2	# MEM &lt;vector(2) long unsigned int&gt; [(uint64_t *)a_156 + ivtmp.458_478 * 1], vect__163.334
</span></span><span class=line><span class=cl>  movdqu	(%r12,%rdx), %xmm9	# MEM &lt;vector(2) long unsigned int&gt; [(uint64_t *)b_159 + ivtmp.458_478 * 1], tmp1968
</span></span><span class=line><span class=cl>  psubq	%xmm9, %xmm2	# tmp1968, vect__163.334
</span></span><span class=line><span class=cl># sub.c:204:         result[i] = a[i] - b[i];
</span></span><span class=line><span class=cl>  movups	%xmm2, (%r14,%rdx)	# vect__163.334, MEM &lt;vector(2) long unsigned int&gt; [(uint64_t *)result_133 + ivtmp.458_478 * 1]
</span></span><span class=line><span class=cl># sub.c:213:             borrow_array[i] = 0;
</span></span><span class=line><span class=cl>  movups	%xmm10, (%rcx,%rdx)	# tmp928, MEM &lt;vector(2) long unsigned int&gt; [(uint64_t *)borrow_array_152 + ivtmp.458_478 * 1]
</span></span><span class=line><span class=cl>  addq	$16, %rdx	#, ivtmp.458
</span></span><span class=line><span class=cl>  cmpq	%r8, %rdx	# _485, ivtmp.458
</span></span><span class=line><span class=cl>  je	.L716	#,
</span></span></code></pre></td></tr></table></div></div></li><li>Case 2: [Without <code>-O3</code>, with <code>-mavx512f</code>]<div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-1-1><a class=lnlinks href=#hl-1-1>1</a>
</span><span class=lnt id=hl-1-2><a class=lnlinks href=#hl-1-2>2</a>
</span><span class=lnt id=hl-1-3><a class=lnlinks href=#hl-1-3>3</a>
</span><span class=lnt id=hl-1-4><a class=lnlinks href=#hl-1-4>4</a>
</span><span class=lnt id=hl-1-5><a class=lnlinks href=#hl-1-5>5</a>
</span><span class=lnt id=hl-1-6><a class=lnlinks href=#hl-1-6>6</a>
</span><span class=lnt id=hl-1-7><a class=lnlinks href=#hl-1-7>7</a>
</span><span class=lnt id=hl-1-8><a class=lnlinks href=#hl-1-8>8</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>  .L126:
</span></span><span class=line><span class=cl># sub.c:204:         result[i] = a[i] - b[i];
</span></span><span class=line><span class=cl>  movl	-1584(%rbp), %eax	# i, tmp360
</span></span><span class=line><span class=cl>  cltq
</span></span><span class=line><span class=cl>  leaq	0(,%rax,8), %rdx	#, _224
</span></span><span class=line><span class=cl>  movq	-1320(%rbp), %rax	# a, tmp361
</span></span><span class=line><span class=cl>  addq	%rdx, %rax	# _224, _226
</span></span><span class=line><span class=cl>  movq	(%rax), %rdx	# *_226, _227
</span></span></code></pre></td></tr></table></div></div></li><li>Case 3: [<code>-O3 -mavx512f</code>]<div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-2-1><a class=lnlinks href=#hl-2-1>1</a>
</span><span class=lnt id=hl-2-2><a class=lnlinks href=#hl-2-2>2</a>
</span><span class=lnt id=hl-2-3><a class=lnlinks href=#hl-2-3>3</a>
</span><span class=lnt id=hl-2-4><a class=lnlinks href=#hl-2-4>4</a>
</span><span class=lnt id=hl-2-5><a class=lnlinks href=#hl-2-5>5</a>
</span><span class=lnt id=hl-2-6><a class=lnlinks href=#hl-2-6>6</a>
</span><span class=lnt id=hl-2-7><a class=lnlinks href=#hl-2-7>7</a>
</span><span class=lnt id=hl-2-8><a class=lnlinks href=#hl-2-8>8</a>
</span><span class=lnt id=hl-2-9><a class=lnlinks href=#hl-2-9>9</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>  .L599:
</span></span><span class=line><span class=cl># sub.c:204:         result[i] = a[i] - b[i];
</span></span><span class=line><span class=cl>  vmovdqu64	(%r14,%rax), %zmm6	# MEM &lt;vector(8) long unsigned int&gt; [(uint64_t *)a_156 + ivtmp.561_842 * 1], tmp2081
</span></span><span class=line><span class=cl>  vpsubq	(%r12,%rax), %zmm6, %zmm11	# MEM &lt;vector(8) long unsigned int&gt; [(uint64_t *)b_159 + ivtmp.561_842 * 1], tmp2081, vect__163.380
</span></span><span class=line><span class=cl># sub.c:204:         result[i] = a[i] - b[i];
</span></span><span class=line><span class=cl>  vmovdqu64	%zmm11, (%r15,%rax)	# vect__163.380, MEM &lt;vector(8) long unsigned int&gt; [(uint64_t *)result_133 + ivtmp.561_842 * 1]
</span></span><span class=line><span class=cl># sub.c:213:             borrow_array[i] = 0;
</span></span><span class=line><span class=cl>  vmovdqu64	%zmm8, (%rcx,%rax)	# tmp1164, MEM &lt;vector(8) long unsigned int&gt; [(uint64_t *)borrow_array_152 + ivtmp.561_842 * 1]
</span></span><span class=line><span class=cl>  addq	$64, %rax	#, ivtmp.561
</span></span></code></pre></td></tr></table></div></div></li></ul><p>For the manual vectorized code, the assembly code generated is as follows:</p><ul><li>Case 4: [With only <code>-mavx512f</code>]<div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-3-1><a class=lnlinks href=#hl-3-1> 1</a>
</span><span class=lnt id=hl-3-2><a class=lnlinks href=#hl-3-2> 2</a>
</span><span class=lnt id=hl-3-3><a class=lnlinks href=#hl-3-3> 3</a>
</span><span class=lnt id=hl-3-4><a class=lnlinks href=#hl-3-4> 4</a>
</span><span class=lnt id=hl-3-5><a class=lnlinks href=#hl-3-5> 5</a>
</span><span class=lnt id=hl-3-6><a class=lnlinks href=#hl-3-6> 6</a>
</span><span class=lnt id=hl-3-7><a class=lnlinks href=#hl-3-7> 7</a>
</span><span class=lnt id=hl-3-8><a class=lnlinks href=#hl-3-8> 8</a>
</span><span class=lnt id=hl-3-9><a class=lnlinks href=#hl-3-9> 9</a>
</span><span class=lnt id=hl-3-10><a class=lnlinks href=#hl-3-10>10</a>
</span><span class=lnt id=hl-3-11><a class=lnlinks href=#hl-3-11>11</a>
</span><span class=lnt id=hl-3-12><a class=lnlinks href=#hl-3-12>12</a>
</span><span class=lnt id=hl-3-13><a class=lnlinks href=#hl-3-13>13</a>
</span><span class=lnt id=hl-3-14><a class=lnlinks href=#hl-3-14>14</a>
</span><span class=lnt id=hl-3-15><a class=lnlinks href=#hl-3-15>15</a>
</span><span class=lnt id=hl-3-16><a class=lnlinks href=#hl-3-16>16</a>
</span><span class=lnt id=hl-3-17><a class=lnlinks href=#hl-3-17>17</a>
</span><span class=lnt id=hl-3-18><a class=lnlinks href=#hl-3-18>18</a>
</span><span class=lnt id=hl-3-19><a class=lnlinks href=#hl-3-19>19</a>
</span><span class=lnt id=hl-3-20><a class=lnlinks href=#hl-3-20>20</a>
</span><span class=lnt id=hl-3-21><a class=lnlinks href=#hl-3-21>21</a>
</span><span class=lnt id=hl-3-22><a class=lnlinks href=#hl-3-22>22</a>
</span><span class=lnt id=hl-3-23><a class=lnlinks href=#hl-3-23>23</a>
</span><span class=lnt id=hl-3-24><a class=lnlinks href=#hl-3-24>24</a>
</span><span class=lnt id=hl-3-25><a class=lnlinks href=#hl-3-25>25</a>
</span><span class=lnt id=hl-3-26><a class=lnlinks href=#hl-3-26>26</a>
</span><span class=lnt id=hl-3-27><a class=lnlinks href=#hl-3-27>27</a>
</span><span class=lnt id=hl-3-28><a class=lnlinks href=#hl-3-28>28</a>
</span><span class=lnt id=hl-3-29><a class=lnlinks href=#hl-3-29>29</a>
</span><span class=lnt id=hl-3-30><a class=lnlinks href=#hl-3-30>30</a>
</span><span class=lnt id=hl-3-31><a class=lnlinks href=#hl-3-31>31</a>
</span><span class=lnt id=hl-3-32><a class=lnlinks href=#hl-3-32>32</a>
</span><span class=lnt id=hl-3-33><a class=lnlinks href=#hl-3-33>33</a>
</span><span class=lnt id=hl-3-34><a class=lnlinks href=#hl-3-34>34</a>
</span><span class=lnt id=hl-3-35><a class=lnlinks href=#hl-3-35>35</a>
</span><span class=lnt id=hl-3-36><a class=lnlinks href=#hl-3-36>36</a>
</span><span class=lnt id=hl-3-37><a class=lnlinks href=#hl-3-37>37</a>
</span><span class=lnt id=hl-3-38><a class=lnlinks href=#hl-3-38>38</a>
</span><span class=lnt id=hl-3-39><a class=lnlinks href=#hl-3-39>39</a>
</span><span class=lnt id=hl-3-40><a class=lnlinks href=#hl-3-40>40</a>
</span><span class=lnt id=hl-3-41><a class=lnlinks href=#hl-3-41>41</a>
</span><span class=lnt id=hl-3-42><a class=lnlinks href=#hl-3-42>42</a>
</span><span class=lnt id=hl-3-43><a class=lnlinks href=#hl-3-43>43</a>
</span><span class=lnt id=hl-3-44><a class=lnlinks href=#hl-3-44>44</a>
</span><span class=lnt id=hl-3-45><a class=lnlinks href=#hl-3-45>45</a>
</span><span class=lnt id=hl-3-46><a class=lnlinks href=#hl-3-46>46</a>
</span><span class=lnt id=hl-3-47><a class=lnlinks href=#hl-3-47>47</a>
</span><span class=lnt id=hl-3-48><a class=lnlinks href=#hl-3-48>48</a>
</span><span class=lnt id=hl-3-49><a class=lnlinks href=#hl-3-49>49</a>
</span><span class=lnt id=hl-3-50><a class=lnlinks href=#hl-3-50>50</a>
</span><span class=lnt id=hl-3-51><a class=lnlinks href=#hl-3-51>51</a>
</span><span class=lnt id=hl-3-52><a class=lnlinks href=#hl-3-52>52</a>
</span><span class=lnt id=hl-3-53><a class=lnlinks href=#hl-3-53>53</a>
</span><span class=lnt id=hl-3-54><a class=lnlinks href=#hl-3-54>54</a>
</span><span class=lnt id=hl-3-55><a class=lnlinks href=#hl-3-55>55</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-gdscript3 data-lang=gdscript3><span class=line><span class=cl>  <span class=c1># sub_avx_64_aligned.c:218:     for (i = 0; i &lt; n; i += 8)</span>
</span></span><span class=line><span class=cl>  <span class=n>movl</span>	<span class=o>$</span><span class=mi>0</span><span class=p>,</span> <span class=o>-</span><span class=mi>3448</span><span class=p>(</span><span class=o>%</span><span class=n>rbp</span><span class=p>)</span>	<span class=c1>#, i</span>
</span></span><span class=line><span class=cl>  <span class=c1># sub_avx_64_aligned.c:218:     for (i = 0; i &lt; n; i += 8)</span>
</span></span><span class=line><span class=cl>    <span class=n>jmp</span>	<span class=o>.</span><span class=n>L137</span>	<span class=c1>#</span>
</span></span><span class=line><span class=cl>  <span class=o>.</span><span class=n>L145</span><span class=p>:</span>
</span></span><span class=line><span class=cl>  <span class=c1># sub_avx_64_aligned.c:222:         a_vec = _mm512_load_si512(a + i);</span>
</span></span><span class=line><span class=cl>    <span class=n>movl</span>	<span class=o>-</span><span class=mi>3448</span><span class=p>(</span><span class=o>%</span><span class=n>rbp</span><span class=p>),</span> <span class=o>%</span><span class=n>eax</span>	<span class=c1># i, tmp394</span>
</span></span><span class=line><span class=cl>    <span class=n>cltq</span>
</span></span><span class=line><span class=cl>    <span class=n>leaq</span>	<span class=mi>0</span><span class=p>(,</span><span class=o>%</span><span class=n>rax</span><span class=p>,</span><span class=mi>8</span><span class=p>),</span> <span class=o>%</span><span class=n>rdx</span>	<span class=c1>#, _242</span>
</span></span><span class=line><span class=cl>  <span class=c1># sub_avx_64_aligned.c:222:         a_vec = _mm512_load_si512(a + i);</span>
</span></span><span class=line><span class=cl>    <span class=n>movq</span>	<span class=o>-</span><span class=mi>2720</span><span class=p>(</span><span class=o>%</span><span class=n>rbp</span><span class=p>),</span> <span class=o>%</span><span class=n>rax</span>	<span class=c1># a, tmp395</span>
</span></span><span class=line><span class=cl>    <span class=n>addq</span>	<span class=o>%</span><span class=n>rdx</span><span class=p>,</span> <span class=o>%</span><span class=n>rax</span>	<span class=c1># _242, _243</span>
</span></span><span class=line><span class=cl>    <span class=n>movq</span>	<span class=o>%</span><span class=n>rax</span><span class=p>,</span> <span class=o>-</span><span class=mi>2664</span><span class=p>(</span><span class=o>%</span><span class=n>rbp</span><span class=p>)</span>	<span class=c1># _243, __P</span>
</span></span><span class=line><span class=cl>  <span class=c1># /usr/lib/gcc/x86_64-linux-gnu/11/include/avx512fintrin.h:542:   return *(__m512i *) __P;</span>
</span></span><span class=line><span class=cl>    <span class=n>movq</span>	<span class=o>-</span><span class=mi>2664</span><span class=p>(</span><span class=o>%</span><span class=n>rbp</span><span class=p>),</span> <span class=o>%</span><span class=n>rax</span>	<span class=c1># __P, tmp396</span>
</span></span><span class=line><span class=cl>    <span class=n>vmovdqa64</span>	<span class=p>(</span><span class=o>%</span><span class=n>rax</span><span class=p>),</span> <span class=o>%</span><span class=n>zmm0</span>	<span class=c1># MEM[(__m512i * {ref-all})__P_244], _245</span>
</span></span><span class=line><span class=cl>  <span class=c1># sub_avx_64_aligned.c:222:         a_vec = _mm512_load_si512(a + i);</span>
</span></span><span class=line><span class=cl>    <span class=n>vmovdqa64</span>	<span class=o>%</span><span class=n>zmm0</span><span class=p>,</span> <span class=o>-</span><span class=mi>2544</span><span class=p>(</span><span class=o>%</span><span class=n>rbp</span><span class=p>)</span>	<span class=c1># _245, a_vec</span>
</span></span><span class=line><span class=cl>  <span class=c1># sub_avx_64_aligned.c:223:         b_vec = _mm512_load_si512(b + i);</span>
</span></span><span class=line><span class=cl>    <span class=n>movl</span>	<span class=o>-</span><span class=mi>3448</span><span class=p>(</span><span class=o>%</span><span class=n>rbp</span><span class=p>),</span> <span class=o>%</span><span class=n>eax</span>	<span class=c1># i, tmp397</span>
</span></span><span class=line><span class=cl>    <span class=n>cltq</span>
</span></span><span class=line><span class=cl>    <span class=n>leaq</span>	<span class=mi>0</span><span class=p>(,</span><span class=o>%</span><span class=n>rax</span><span class=p>,</span><span class=mi>8</span><span class=p>),</span> <span class=o>%</span><span class=n>rdx</span>	<span class=c1>#, _249</span>
</span></span><span class=line><span class=cl>  <span class=c1># sub_avx_64_aligned.c:223:         b_vec = _mm512_load_si512(b + i);</span>
</span></span><span class=line><span class=cl>    <span class=n>movq</span>	<span class=o>-</span><span class=mi>2712</span><span class=p>(</span><span class=o>%</span><span class=n>rbp</span><span class=p>),</span> <span class=o>%</span><span class=n>rax</span>	<span class=c1># b, tmp398</span>
</span></span><span class=line><span class=cl>    <span class=n>addq</span>	<span class=o>%</span><span class=n>rdx</span><span class=p>,</span> <span class=o>%</span><span class=n>rax</span>	<span class=c1># _249, _250</span>
</span></span><span class=line><span class=cl>    <span class=n>movq</span>	<span class=o>%</span><span class=n>rax</span><span class=p>,</span> <span class=o>-</span><span class=mi>2656</span><span class=p>(</span><span class=o>%</span><span class=n>rbp</span><span class=p>)</span>	<span class=c1># _250, __P</span>
</span></span><span class=line><span class=cl>  <span class=c1># /usr/lib/gcc/x86_64-linux-gnu/11/include/avx512fintrin.h:542:   return *(__m512i *) __P;</span>
</span></span><span class=line><span class=cl>    <span class=n>movq</span>	<span class=o>-</span><span class=mi>2656</span><span class=p>(</span><span class=o>%</span><span class=n>rbp</span><span class=p>),</span> <span class=o>%</span><span class=n>rax</span>	<span class=c1># __P, tmp399</span>
</span></span><span class=line><span class=cl>    <span class=n>vmovdqa64</span>	<span class=p>(</span><span class=o>%</span><span class=n>rax</span><span class=p>),</span> <span class=o>%</span><span class=n>zmm0</span>	<span class=c1># MEM[(__m512i * {ref-all})__P_251], _252</span>
</span></span><span class=line><span class=cl>  <span class=c1># sub_avx_64_aligned.c:223:         b_vec = _mm512_load_si512(b + i);</span>
</span></span><span class=line><span class=cl>    <span class=n>vmovdqa64</span>	<span class=o>%</span><span class=n>zmm0</span><span class=p>,</span> <span class=o>-</span><span class=mi>2480</span><span class=p>(</span><span class=o>%</span><span class=n>rbp</span><span class=p>)</span>	<span class=c1># _252, b_vec</span>
</span></span><span class=line><span class=cl>    <span class=n>vmovdqa64</span>	<span class=o>-</span><span class=mi>2544</span><span class=p>(</span><span class=o>%</span><span class=n>rbp</span><span class=p>),</span> <span class=o>%</span><span class=n>zmm0</span>	<span class=c1># a_vec, tmp400</span>
</span></span><span class=line><span class=cl>    <span class=n>vmovdqa64</span>	<span class=o>%</span><span class=n>zmm0</span><span class=p>,</span> <span class=o>-</span><span class=mi>2416</span><span class=p>(</span><span class=o>%</span><span class=n>rbp</span><span class=p>)</span>	<span class=c1># tmp400, __A</span>
</span></span><span class=line><span class=cl>    <span class=n>vmovdqa64</span>	<span class=o>-</span><span class=mi>2480</span><span class=p>(</span><span class=o>%</span><span class=n>rbp</span><span class=p>),</span> <span class=o>%</span><span class=n>zmm0</span>	<span class=c1># b_vec, tmp401</span>
</span></span><span class=line><span class=cl>    <span class=n>vmovdqa64</span>	<span class=o>%</span><span class=n>zmm0</span><span class=p>,</span> <span class=o>-</span><span class=mi>2352</span><span class=p>(</span><span class=o>%</span><span class=n>rbp</span><span class=p>)</span>	<span class=c1># tmp401, __B</span>
</span></span><span class=line><span class=cl>  <span class=c1># /usr/lib/gcc/x86_64-linux-gnu/11/include/avx512fintrin.h:762:   return (__m512i) ((__v8du) __A - (__v8du) __B);</span>
</span></span><span class=line><span class=cl>    <span class=n>vmovdqa64</span>	<span class=o>-</span><span class=mi>2416</span><span class=p>(</span><span class=o>%</span><span class=n>rbp</span><span class=p>),</span> <span class=o>%</span><span class=n>zmm0</span>	<span class=c1># __A, _257</span>
</span></span><span class=line><span class=cl>  <span class=c1># /usr/lib/gcc/x86_64-linux-gnu/11/include/avx512fintrin.h:762:   return (__m512i) ((__v8du) __A - (__v8du) __B);</span>
</span></span><span class=line><span class=cl>    <span class=n>vmovdqa64</span>	<span class=o>-</span><span class=mi>2352</span><span class=p>(</span><span class=o>%</span><span class=n>rbp</span><span class=p>),</span> <span class=o>%</span><span class=n>zmm1</span>	<span class=c1># __B, _258</span>
</span></span><span class=line><span class=cl>  <span class=c1># /usr/lib/gcc/x86_64-linux-gnu/11/include/avx512fintrin.h:762:   return (__m512i) ((__v8du) __A - (__v8du) __B);</span>
</span></span><span class=line><span class=cl>    <span class=n>vpsubq</span>	<span class=o>%</span><span class=n>zmm1</span><span class=p>,</span> <span class=o>%</span><span class=n>zmm0</span><span class=p>,</span> <span class=o>%</span><span class=n>zmm0</span>	<span class=c1># _258, _257, _259</span>
</span></span><span class=line><span class=cl>  <span class=c1># sub_avx_64_aligned.c:227:         result_vec = _mm512_sub_epi64(a_vec, b_vec);</span>
</span></span><span class=line><span class=cl>    <span class=n>vmovdqa64</span>	<span class=o>%</span><span class=n>zmm0</span><span class=p>,</span> <span class=o>-</span><span class=mi>2288</span><span class=p>(</span><span class=o>%</span><span class=n>rbp</span><span class=p>)</span>	<span class=c1># _260, result_vec</span>
</span></span><span class=line><span class=cl>  <span class=c1># sub_avx_64_aligned.c:231:         __mmask8 borrow_mask = _mm512_cmplt_epi64_mask(result_vec, zeros);</span>
</span></span><span class=line><span class=cl>    <span class=n>vmovdqa64</span>	<span class=n>zeros</span><span class=p>(</span><span class=o>%</span><span class=n>rip</span><span class=p>),</span> <span class=o>%</span><span class=n>zmm0</span>	<span class=c1># zeros, zeros.47_263</span>
</span></span><span class=line><span class=cl>    <span class=n>vmovdqa64</span>	<span class=o>-</span><span class=mi>2288</span><span class=p>(</span><span class=o>%</span><span class=n>rbp</span><span class=p>),</span> <span class=o>%</span><span class=n>zmm1</span>	<span class=c1># result_vec, tmp402</span>
</span></span><span class=line><span class=cl>    <span class=n>vmovdqa64</span>	<span class=o>%</span><span class=n>zmm1</span><span class=p>,</span> <span class=o>-</span><span class=mi>2224</span><span class=p>(</span><span class=o>%</span><span class=n>rbp</span><span class=p>)</span>	<span class=c1># tmp402, __X</span>
</span></span><span class=line><span class=cl>    <span class=n>vmovdqa64</span>	<span class=o>%</span><span class=n>zmm0</span><span class=p>,</span> <span class=o>-</span><span class=mi>2160</span><span class=p>(</span><span class=o>%</span><span class=n>rbp</span><span class=p>)</span>	<span class=c1># zeros.47_263, __Y</span>
</span></span><span class=line><span class=cl>  <span class=c1># /usr/lib/gcc/x86_64-linux-gnu/11/include/avx512fintrin.h:9896:   return (__mmask8) __builtin_ia32_cmpq512_mask ((__v8di) __X,</span>
</span></span><span class=line><span class=cl>    <span class=n>vmovdqa64</span>	<span class=o>-</span><span class=mi>2224</span><span class=p>(</span><span class=o>%</span><span class=n>rbp</span><span class=p>),</span> <span class=o>%</span><span class=n>zmm0</span>	<span class=c1># __X, tmp403</span>
</span></span><span class=line><span class=cl>    <span class=n>vmovdqa64</span>	<span class=o>-</span><span class=mi>2160</span><span class=p>(</span><span class=o>%</span><span class=n>rbp</span><span class=p>),</span> <span class=o>%</span><span class=n>zmm1</span>	<span class=c1># __Y, tmp404</span>
</span></span><span class=line><span class=cl>    <span class=n>movl</span>	<span class=o>$-</span><span class=mi>1</span><span class=p>,</span> <span class=o>%</span><span class=n>eax</span>	<span class=c1>#, tmp405</span>
</span></span><span class=line><span class=cl>    <span class=n>kmovw</span>	<span class=o>%</span><span class=n>eax</span><span class=p>,</span> <span class=o>%</span><span class=n>k1</span>	<span class=c1># tmp405, tmp405</span>
</span></span><span class=line><span class=cl>    <span class=n>vpcmpq</span>	<span class=o>$</span><span class=mi>1</span><span class=p>,</span> <span class=o>%</span><span class=n>zmm1</span><span class=p>,</span> <span class=o>%</span><span class=n>zmm0</span><span class=p>,</span> <span class=o>%</span><span class=n>k0</span><span class=p>{</span><span class=o>%</span><span class=n>k1</span><span class=p>}</span>	<span class=c1>#, tmp404, tmp403, _266, tmp405</span>
</span></span><span class=line><span class=cl>    <span class=n>kmovw</span>	<span class=o>%</span><span class=n>k0</span><span class=p>,</span> <span class=o>%</span><span class=n>eax</span>	<span class=c1># _266, _266</span>
</span></span></code></pre></td></tr></table></div></div></li><li>Case 5: [<code>-O3 -mavx512f</code>]<div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-4-1><a class=lnlinks href=#hl-4-1> 1</a>
</span><span class=lnt id=hl-4-2><a class=lnlinks href=#hl-4-2> 2</a>
</span><span class=lnt id=hl-4-3><a class=lnlinks href=#hl-4-3> 3</a>
</span><span class=lnt id=hl-4-4><a class=lnlinks href=#hl-4-4> 4</a>
</span><span class=lnt id=hl-4-5><a class=lnlinks href=#hl-4-5> 5</a>
</span><span class=lnt id=hl-4-6><a class=lnlinks href=#hl-4-6> 6</a>
</span><span class=lnt id=hl-4-7><a class=lnlinks href=#hl-4-7> 7</a>
</span><span class=lnt id=hl-4-8><a class=lnlinks href=#hl-4-8> 8</a>
</span><span class=lnt id=hl-4-9><a class=lnlinks href=#hl-4-9> 9</a>
</span><span class=lnt id=hl-4-10><a class=lnlinks href=#hl-4-10>10</a>
</span><span class=lnt id=hl-4-11><a class=lnlinks href=#hl-4-11>11</a>
</span><span class=lnt id=hl-4-12><a class=lnlinks href=#hl-4-12>12</a>
</span><span class=lnt id=hl-4-13><a class=lnlinks href=#hl-4-13>13</a>
</span><span class=lnt id=hl-4-14><a class=lnlinks href=#hl-4-14>14</a>
</span><span class=lnt id=hl-4-15><a class=lnlinks href=#hl-4-15>15</a>
</span><span class=lnt id=hl-4-16><a class=lnlinks href=#hl-4-16>16</a>
</span><span class=lnt id=hl-4-17><a class=lnlinks href=#hl-4-17>17</a>
</span><span class=lnt id=hl-4-18><a class=lnlinks href=#hl-4-18>18</a>
</span><span class=lnt id=hl-4-19><a class=lnlinks href=#hl-4-19>19</a>
</span><span class=lnt id=hl-4-20><a class=lnlinks href=#hl-4-20>20</a>
</span><span class=lnt id=hl-4-21><a class=lnlinks href=#hl-4-21>21</a>
</span><span class=lnt id=hl-4-22><a class=lnlinks href=#hl-4-22>22</a>
</span><span class=lnt id=hl-4-23><a class=lnlinks href=#hl-4-23>23</a>
</span><span class=lnt id=hl-4-24><a class=lnlinks href=#hl-4-24>24</a>
</span><span class=lnt id=hl-4-25><a class=lnlinks href=#hl-4-25>25</a>
</span><span class=lnt id=hl-4-26><a class=lnlinks href=#hl-4-26>26</a>
</span><span class=lnt id=hl-4-27><a class=lnlinks href=#hl-4-27>27</a>
</span><span class=lnt id=hl-4-28><a class=lnlinks href=#hl-4-28>28</a>
</span><span class=lnt id=hl-4-29><a class=lnlinks href=#hl-4-29>29</a>
</span><span class=lnt id=hl-4-30><a class=lnlinks href=#hl-4-30>30</a>
</span><span class=lnt id=hl-4-31><a class=lnlinks href=#hl-4-31>31</a>
</span><span class=lnt id=hl-4-32><a class=lnlinks href=#hl-4-32>32</a>
</span><span class=lnt id=hl-4-33><a class=lnlinks href=#hl-4-33>33</a>
</span><span class=lnt id=hl-4-34><a class=lnlinks href=#hl-4-34>34</a>
</span><span class=lnt id=hl-4-35><a class=lnlinks href=#hl-4-35>35</a>
</span><span class=lnt id=hl-4-36><a class=lnlinks href=#hl-4-36>36</a>
</span><span class=lnt id=hl-4-37><a class=lnlinks href=#hl-4-37>37</a>
</span><span class=lnt id=hl-4-38><a class=lnlinks href=#hl-4-38>38</a>
</span><span class=lnt id=hl-4-39><a class=lnlinks href=#hl-4-39>39</a>
</span><span class=lnt id=hl-4-40><a class=lnlinks href=#hl-4-40>40</a>
</span><span class=lnt id=hl-4-41><a class=lnlinks href=#hl-4-41>41</a>
</span><span class=lnt id=hl-4-42><a class=lnlinks href=#hl-4-42>42</a>
</span><span class=lnt id=hl-4-43><a class=lnlinks href=#hl-4-43>43</a>
</span><span class=lnt id=hl-4-44><a class=lnlinks href=#hl-4-44>44</a>
</span><span class=lnt id=hl-4-45><a class=lnlinks href=#hl-4-45>45</a>
</span><span class=lnt id=hl-4-46><a class=lnlinks href=#hl-4-46>46</a>
</span><span class=lnt id=hl-4-47><a class=lnlinks href=#hl-4-47>47</a>
</span><span class=lnt id=hl-4-48><a class=lnlinks href=#hl-4-48>48</a>
</span><span class=lnt id=hl-4-49><a class=lnlinks href=#hl-4-49>49</a>
</span><span class=lnt id=hl-4-50><a class=lnlinks href=#hl-4-50>50</a>
</span><span class=lnt id=hl-4-51><a class=lnlinks href=#hl-4-51>51</a>
</span><span class=lnt id=hl-4-52><a class=lnlinks href=#hl-4-52>52</a>
</span><span class=lnt id=hl-4-53><a class=lnlinks href=#hl-4-53>53</a>
</span><span class=lnt id=hl-4-54><a class=lnlinks href=#hl-4-54>54</a>
</span><span class=lnt id=hl-4-55><a class=lnlinks href=#hl-4-55>55</a>
</span><span class=lnt id=hl-4-56><a class=lnlinks href=#hl-4-56>56</a>
</span><span class=lnt id=hl-4-57><a class=lnlinks href=#hl-4-57>57</a>
</span><span class=lnt id=hl-4-58><a class=lnlinks href=#hl-4-58>58</a>
</span><span class=lnt id=hl-4-59><a class=lnlinks href=#hl-4-59>59</a>
</span><span class=lnt id=hl-4-60><a class=lnlinks href=#hl-4-60>60</a>
</span><span class=lnt id=hl-4-61><a class=lnlinks href=#hl-4-61>61</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>  .L348:
</span></span><span class=line><span class=cl>  # /usr/lib/gcc/x86_64-linux-gnu/11/include/avx512fintrin.h:762:   return (__m512i) ((__v8du) __A - (__v8du) __B);
</span></span><span class=line><span class=cl>    vmovdqa64	(%r12,%rax,8), %zmm0	# MEM[(__m512i * {ref-all})a_179 + ivtmp.520_1112 * 8], tmp1805
</span></span><span class=line><span class=cl>    vpsubq	(%r10,%rax,8), %zmm0, %zmm3	# MEM[(__m512i * {ref-all})b_182 + ivtmp.520_1112 * 8], tmp1805, tmp1277
</span></span><span class=line><span class=cl>  # sub_avx_64_aligned.c:218:     for (i = 0; i &lt; n; i += 8)
</span></span><span class=line><span class=cl>    leaq	8(%rax), %rdx	#, tmp1209
</span></span><span class=line><span class=cl>    leaq	16(%rax), %r8	#, ivtmp.520
</span></span><span class=line><span class=cl>    leaq	24(%rax), %r11	#, ivtmp.520
</span></span><span class=line><span class=cl>  # /usr/lib/gcc/x86_64-linux-gnu/11/include/avx512fintrin.h:9896:   return (__mmask8) __builtin_ia32_cmpq512_mask ((__v8di) __X,
</span></span><span class=line><span class=cl>    vpcmpq	$1, zeros(%rip), %zmm3, %k5	#, zeros, tmp1277, tmp1279
</span></span><span class=line><span class=cl>  # /usr/lib/gcc/x86_64-linux-gnu/11/include/avx512fintrin.h:741:   return (__m512i) __builtin_ia32_paddq512_mask ((__v8di) __A,
</span></span><span class=line><span class=cl>    vmovdqa64	%zmm3, %zmm4	# tmp1277, tmp1277
</span></span><span class=line><span class=cl>    vpaddq	limb_digits(%rip), %zmm3, %zmm4{%k5}	# limb_digits, tmp1277, tmp1277, tmp1279, tmp1277
</span></span><span class=line><span class=cl>  # /usr/lib/gcc/x86_64-linux-gnu/11/include/avx512fintrin.h:4201: 	 __builtin_ia32_pbroadcastq512_gpr_mask (__A,
</span></span><span class=line><span class=cl>    vpbroadcastq	%r9, %zmm8{%k5}{z}	# tmp1057, tmp1282, tmp1279,
</span></span><span class=line><span class=cl>  # /usr/lib/gcc/x86_64-linux-gnu/11/include/avx512fintrin.h:575:   *(__m512i *) __P = __A;
</span></span><span class=line><span class=cl>    vmovdqa64	%zmm8, (%rdi,%rax,8)	# tmp1282, MEM[(__m512i * {ref-all})borrow_array_175 + ivtmp.520_1112 * 8]
</span></span><span class=line><span class=cl>    vmovdqa64	%zmm4, (%rbx,%rax,8)	# tmp1281, MEM[(__m512i * {ref-all})result_156 + ivtmp.520_1112 * 8]
</span></span><span class=line><span class=cl>  # /usr/lib/gcc/x86_64-linux-gnu/11/include/avx512fintrin.h:762:   return (__m512i) ((__v8du) __A - (__v8du) __B);
</span></span><span class=line><span class=cl>    vmovdqa64	(%r12,%rdx,8), %zmm9	# MEM[(__m512i * {ref-all})a_179 + ivtmp.520_1112 * 8], tmp1807
</span></span><span class=line><span class=cl>  # sub_avx_64_aligned.c:218:     for (i = 0; i &lt; n; i += 8)
</span></span><span class=line><span class=cl>    addq	$32, %rax	#, ivtmp.520
</span></span><span class=line><span class=cl>  # /usr/lib/gcc/x86_64-linux-gnu/11/include/avx512fintrin.h:762:   return (__m512i) ((__v8du) __A - (__v8du) __B);
</span></span><span class=line><span class=cl>    vpsubq	(%r10,%rdx,8), %zmm9, %zmm1	# MEM[(__m512i * {ref-all})b_182 + ivtmp.520_1112 * 8], tmp1807, tmp1285
</span></span><span class=line><span class=cl>  # /usr/lib/gcc/x86_64-linux-gnu/11/include/avx512fintrin.h:9896:   return (__mmask8) __builtin_ia32_cmpq512_mask ((__v8di) __X,
</span></span><span class=line><span class=cl>    vpcmpq	$1, zeros(%rip), %zmm1, %k6	#, zeros, tmp1285, tmp1287
</span></span><span class=line><span class=cl>  # /usr/lib/gcc/x86_64-linux-gnu/11/include/avx512fintrin.h:741:   return (__m512i) __builtin_ia32_paddq512_mask ((__v8di) __A,
</span></span><span class=line><span class=cl>    vpaddq	limb_digits(%rip), %zmm1, %zmm1{%k6}	# limb_digits, tmp1285, tmp1285, tmp1287, tmp1285
</span></span><span class=line><span class=cl>  # /usr/lib/gcc/x86_64-linux-gnu/11/include/avx512fintrin.h:4201: 	 __builtin_ia32_pbroadcastq512_gpr_mask (__A,
</span></span><span class=line><span class=cl>    vpbroadcastq	%r9, %zmm5{%k6}{z}	# tmp1057, tmp1290, tmp1287,
</span></span><span class=line><span class=cl>  # /usr/lib/gcc/x86_64-linux-gnu/11/include/avx512fintrin.h:575:   *(__m512i *) __P = __A;
</span></span><span class=line><span class=cl>    vmovdqa64	%zmm5, (%rdi,%rdx,8)	# tmp1290, MEM[(__m512i * {ref-all})borrow_array_175 + ivtmp.520_1112 * 8]
</span></span><span class=line><span class=cl>    vmovdqa64	%zmm1, (%rbx,%rdx,8)	# tmp1289, MEM[(__m512i * {ref-all})result_156 + ivtmp.520_1112 * 8]
</span></span><span class=line><span class=cl>  # /usr/lib/gcc/x86_64-linux-gnu/11/include/avx512fintrin.h:762:   return (__m512i) ((__v8du) __A - (__v8du) __B);
</span></span><span class=line><span class=cl>    vmovdqa64	(%r12,%r8,8), %zmm11	# MEM[(__m512i * {ref-all})a_179 + ivtmp.520_1112 * 8], tmp1809
</span></span><span class=line><span class=cl>    vpsubq	(%r10,%r8,8), %zmm11, %zmm12	# MEM[(__m512i * {ref-all})b_182 + ivtmp.520_1112 * 8], tmp1809, tmp1293
</span></span><span class=line><span class=cl>  # /usr/lib/gcc/x86_64-linux-gnu/11/include/avx512fintrin.h:9896:   return (__mmask8) __builtin_ia32_cmpq512_mask ((__v8di) __X,
</span></span><span class=line><span class=cl>    vpcmpq	$1, zeros(%rip), %zmm12, %k7	#, zeros, tmp1293, tmp1295
</span></span><span class=line><span class=cl>  # /usr/lib/gcc/x86_64-linux-gnu/11/include/avx512fintrin.h:741:   return (__m512i) __builtin_ia32_paddq512_mask ((__v8di) __A,
</span></span><span class=line><span class=cl>    vmovdqa64	%zmm12, %zmm10	# tmp1293, tmp1293
</span></span><span class=line><span class=cl>    vpaddq	limb_digits(%rip), %zmm12, %zmm10{%k7}	# limb_digits, tmp1293, tmp1293, tmp1295, tmp1293
</span></span><span class=line><span class=cl>  # /usr/lib/gcc/x86_64-linux-gnu/11/include/avx512fintrin.h:4201: 	 __builtin_ia32_pbroadcastq512_gpr_mask (__A,
</span></span><span class=line><span class=cl>    vpbroadcastq	%r9, %zmm13{%k7}{z}	# tmp1057, tmp1298, tmp1295,
</span></span><span class=line><span class=cl>  # /usr/lib/gcc/x86_64-linux-gnu/11/include/avx512fintrin.h:575:   *(__m512i *) __P = __A;
</span></span><span class=line><span class=cl>    vmovdqa64	%zmm13, (%rdi,%r8,8)	# tmp1298, MEM[(__m512i * {ref-all})borrow_array_175 + ivtmp.520_1112 * 8]
</span></span><span class=line><span class=cl>    vmovdqa64	%zmm10, (%rbx,%r8,8)	# tmp1297, MEM[(__m512i * {ref-all})result_156 + ivtmp.520_1112 * 8]
</span></span><span class=line><span class=cl>  # /usr/lib/gcc/x86_64-linux-gnu/11/include/avx512fintrin.h:762:   return (__m512i) ((__v8du) __A - (__v8du) __B);
</span></span><span class=line><span class=cl>    vmovdqa64	(%r12,%r11,8), %zmm14	# MEM[(__m512i * {ref-all})a_179 + ivtmp.520_1112 * 8], tmp1811
</span></span><span class=line><span class=cl>    vpsubq	(%r10,%r11,8), %zmm14, %zmm1	# MEM[(__m512i * {ref-all})b_182 + ivtmp.520_1112 * 8], tmp1811, tmp1301
</span></span><span class=line><span class=cl>  # /usr/lib/gcc/x86_64-linux-gnu/11/include/avx512fintrin.h:9896:   return (__mmask8) __builtin_ia32_cmpq512_mask ((__v8di) __X,
</span></span><span class=line><span class=cl>    vpcmpq	$1, zeros(%rip), %zmm1, %k1	#, zeros, tmp1301, tmp1303
</span></span><span class=line><span class=cl>  # /usr/lib/gcc/x86_64-linux-gnu/11/include/avx512fintrin.h:741:   return (__m512i) __builtin_ia32_paddq512_mask ((__v8di) __A,
</span></span><span class=line><span class=cl>    vpaddq	limb_digits(%rip), %zmm1, %zmm1{%k1}	# limb_digits, tmp1301, tmp1301, tmp1303, tmp1301
</span></span><span class=line><span class=cl>  # /usr/lib/gcc/x86_64-linux-gnu/11/include/avx512fintrin.h:4201: 	 __builtin_ia32_pbroadcastq512_gpr_mask (__A,
</span></span><span class=line><span class=cl>    vpbroadcastq	%r9, %zmm7{%k1}{z}	# tmp1057, tmp1306, tmp1303,
</span></span><span class=line><span class=cl>  # /usr/lib/gcc/x86_64-linux-gnu/11/include/avx512fintrin.h:575:   *(__m512i *) __P = __A;
</span></span><span class=line><span class=cl>    vmovdqa64	%zmm7, (%rdi,%r11,8)	# tmp1306, MEM[(__m512i * {ref-all})borrow_array_175 + ivtmp.520_1112 * 8]
</span></span><span class=line><span class=cl>    vmovdqa64	%zmm1, (%rbx,%r11,8)	# tmp1305, MEM[(__m512i * {ref-all})result_156 + ivtmp.520_1112 * 8]
</span></span><span class=line><span class=cl>  # sub_avx_64_aligned.c:218:     for (i = 0; i &lt; n; i += 8)
</span></span><span class=line><span class=cl>    cmpl	%eax, %r14d	# ivtmp.520, n_limb.37_27
</span></span><span class=line><span class=cl>    jg	.L348	#,
</span></span></code></pre></td></tr></table></div></div></li></ul><h3 id=gdb>GDB<a hidden class=anchor aria-hidden=true href=#gdb>#</a></h3><p><a href=https://www.man7.org/linux/man-pages/man1/gdb.1.html>GDB</a> is the GNU Debugger, a widely-used tool that lets you see what&rsquo;s happening inside a program while it runs or what it was doing when it crashed. We&rsquo;ve relied heavily on GDB to debug various issues, trace the program flow of the algorithms we&rsquo;ve implemented, and analyze the assembly code generated after compiler optimizations.</p><h3 id=valgrind>Valgrind<a hidden class=anchor aria-hidden=true href=#valgrind>#</a></h3><p><a href=https://valgrind.org/>Valgrind</a> is a powerful tool for memory debugging, leak detection, and profiling. It has been indispensable in our project for identifying memory leaks and ensuring that our program uses memory efficiently, helping us maintain stability and performance throughout the development process.</p><h2 id=timing-measurements>Timing Measurements<a hidden class=anchor aria-hidden=true href=#timing-measurements>#</a></h2><p>To correctly measure the performance of our algorithms, we need to ensure that our timing mechanisms are accurate and reliable. We&rsquo;ve explored various timing methods to benchmark our code effectively and resorted to using the below technique:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-5-1><a class=lnlinks href=#hl-5-1> 1</a>
</span><span class=lnt id=hl-5-2><a class=lnlinks href=#hl-5-2> 2</a>
</span><span class=lnt id=hl-5-3><a class=lnlinks href=#hl-5-3> 3</a>
</span><span class=lnt id=hl-5-4><a class=lnlinks href=#hl-5-4> 4</a>
</span><span class=lnt id=hl-5-5><a class=lnlinks href=#hl-5-5> 5</a>
</span><span class=lnt id=hl-5-6><a class=lnlinks href=#hl-5-6> 6</a>
</span><span class=lnt id=hl-5-7><a class=lnlinks href=#hl-5-7> 7</a>
</span><span class=lnt id=hl-5-8><a class=lnlinks href=#hl-5-8> 8</a>
</span><span class=lnt id=hl-5-9><a class=lnlinks href=#hl-5-9> 9</a>
</span><span class=lnt id=hl-5-10><a class=lnlinks href=#hl-5-10>10</a>
</span><span class=lnt id=hl-5-11><a class=lnlinks href=#hl-5-11>11</a>
</span><span class=lnt id=hl-5-12><a class=lnlinks href=#hl-5-12>12</a>
</span><span class=lnt id=hl-5-13><a class=lnlinks href=#hl-5-13>13</a>
</span><span class=lnt id=hl-5-14><a class=lnlinks href=#hl-5-14>14</a>
</span><span class=lnt id=hl-5-15><a class=lnlinks href=#hl-5-15>15</a>
</span><span class=lnt id=hl-5-16><a class=lnlinks href=#hl-5-16>16</a>
</span><span class=lnt id=hl-5-17><a class=lnlinks href=#hl-5-17>17</a>
</span><span class=lnt id=hl-5-18><a class=lnlinks href=#hl-5-18>18</a>
</span><span class=lnt id=hl-5-19><a class=lnlinks href=#hl-5-19>19</a>
</span><span class=lnt id=hl-5-20><a class=lnlinks href=#hl-5-20>20</a>
</span><span class=lnt id=hl-5-21><a class=lnlinks href=#hl-5-21>21</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>#define TIME_&lt;measure_type&gt;(t, func)                   \
</span></span><span class=line><span class=cl>    do                                                 \
</span></span><span class=line><span class=cl>    {                                                  \
</span></span><span class=line><span class=cl>        unsigned long long __t0, __t1, __times, __tmp; \
</span></span><span class=line><span class=cl>        __times = 1;                                   \
</span></span><span class=line><span class=cl>        {                                              \
</span></span><span class=line><span class=cl>            func;                                      \
</span></span><span class=line><span class=cl>        }                                              \
</span></span><span class=line><span class=cl>        do                                             \
</span></span><span class=line><span class=cl>        {                                              \
</span></span><span class=line><span class=cl>            __times &lt;&lt;= 1;                             \
</span></span><span class=line><span class=cl>            __t0 = measure_start();              \
</span></span><span class=line><span class=cl>            for (int __t = 0; __t &lt; __times; __t++)    \
</span></span><span class=line><span class=cl>            {                                          \
</span></span><span class=line><span class=cl>                func;                                  \
</span></span><span class=line><span class=cl>            }                                          \
</span></span><span class=line><span class=cl>            __t1 = measure__end();                     \
</span></span><span class=line><span class=cl>            __tmp = __t1 - __t0;                       \
</span></span><span class=line><span class=cl>        } while (__tmp &lt; THRESHOLD);                   \
</span></span><span class=line><span class=cl>        (t) = (double)(__tmp) / __times;               \
</span></span><span class=line><span class=cl>    } while (0)
</span></span></code></pre></td></tr></table></div></div><p>where, <code>&lt;measure_type></code> can be <code>RDTSC</code>, <code>RUSAGE</code>, <code>TIMESPEC</code> etc. and <code>measure_start()</code> and <code>measure_end()</code> are the functions to measure the start and end times, respectively. This macro repeatedly doubles the number of iterations until the measured time exceeds a predefined threshold, ensuring that the timing is accurate and stable. And, returns the average time taken as per the last executed loopin which the threshold is crossed.</p><h2 id=vedic-mathematical-algorithms>Vedic Mathematical Algorithms<a hidden class=anchor aria-hidden=true href=#vedic-mathematical-algorithms>#</a></h2><p>God Resource: <a href=https://archive.org/details/vedic-mathematics-bharati-krishna-tirth-ji-maharaj/page/n17/mode/2up>Vedic Mathematics by Bharati Krishna Tirth Ji Maharaj</a><br>Once we got comfortable with the tools, we shifted focus to tweaking Vedic mathematical algorithms like Urdhva Tiryagbhyam, particularly for multiplication, described in the later part of the post.</p><h2 id=addition>Addition<a hidden class=anchor aria-hidden=true href=#addition>#</a></h2><p>Optimizing addition can be challenging due to the direct dependency of carries in subsequent digit additions when adding two numbers. This dependency may create performance bottlenecks, as each digit addition must wait for the previous carry to be generated before proceeding. Consequently, many libraries do not leverage parallel computation in their addition implementations for large numbers. However, in certain scenarios, the addition of partial digits may not propagate carries to subsequent digit additions. By exploiting this behavior, we might design our addition algorithm such that the additions of digits become independent of each other. This independence could enable us to process the digit additions in parallel, potentially yielding significant performance improvements.</p><p>The key to optimizing addition operations is to partition the digits in a way that minimizes the likelihood of carry propagation in a chained manner. We will approach this by implementing a two-phase process for adding large numbers:</p><ol><li><p><strong>First Phase:</strong> Perform the independent, simultaneous addition of the two digits without initially accounting for carries. This phase aims to complete the digit-wise additions as much as possible in parallel, reducing the immediate dependency on carry propagation.</p></li><li><p><strong>Second Phase:</strong> After the initial addition is complete, identify and account for any carries generated. These carries will then be added to the partial sum result from the first phase.</p></li></ol><p>It is important to note that while the second phase may introduce additional carries that require propagation, our goal is to design the digit partitioning to minimize the probability of such scenarios. In cases where multiple rounds of carry propagation are necessary, we will handle them efficiently to avoid significant performance degradation. This approach aims to leverage advanced processing techniques like AVX or multithreading, thus improving overall performance while managing the complexity introduced by carries.</p><h3 id=grouping-of-digits-for-efficient-processing-for-addition>Grouping of Digits for Efficient Processing for Addition<a hidden class=anchor aria-hidden=true href=#grouping-of-digits-for-efficient-processing-for-addition>#</a></h3><p>When performing arithmetic operations on typical computers, the Instruction Set Architecture (ISA) is designed to process data in 32-bit or 64-bit chunks. To efficiently utilize these data processing units, we can group multiple digits together into a single unit. Specifically, with a 32-bit unit, the maximum value that can be represented in an unsigned format is 4,294,967,295. By grouping nine consecutive digits into a 32-bit data unit, we can safely accommodate potential partial sum overflows during addition operations. This approach reduces the number of arithmetic operations required, as more digits are processed in each operation, thereby improving computational performance.</p><h4 id=example-of-digit-grouping-for-addition>Example of Digit Grouping for Addition<a hidden class=anchor aria-hidden=true href=#example-of-digit-grouping-for-addition>#</a></h4><p>Assume we are grouping nine digits, and let the two numbers <code>x</code> and <code>y</code> be as follows:<br><code>x = 125634987654321987654321987</code><br><code>y = 917683012345678012345678012</code>.<br>Since we are grouping nine digits internally,<br><code>x</code> will be stored as <code>&lt;125634987, 654321987, 654321987></code>,<br>and <code>y</code> will be stored as <code>&lt;917683012, 345678012, 345678012></code>.<br>Additionally, if the number of digits is not a multiple of nine, we will prepend zeros to the numbers.</p><h3 id=avoiding-chained-propagation-of-carries>Avoiding Chained Propagation of Carries<a hidden class=anchor aria-hidden=true href=#avoiding-chained-propagation-of-carries>#</a></h3><p>As we have grouped a certain number of digits together and treated them as a single unit, packing them into a data structure of 32 or 64 bits, we have reduced the likelihood of encountering scenarios that necessitate extensive carry propagation. This grouping approach minimizes the chance of cascading carry dependencies across the entire number and enables more efficient processing.</p><p>For example, if we group digits into 32-bit or 64-bit blocks, the carry propagation within each block is localized, thereby reducing the impact on overall performance. This method allows for parallel processing of multiple blocks, leveraging advanced technologies such as AVX or multithreading. As a result, the algorithm can handle larger numbers more efficiently and maintain high performance despite the inherent complexity of carry management.</p><h4 id=examples-of-chained-carry-propagation>Examples of Chained Carry Propagation<a hidden class=anchor aria-hidden=true href=#examples-of-chained-carry-propagation>#</a></h4><p>Consider the following example:</p><ul><li><strong>Example 1:</strong></li></ul><p><code>a = 999...9999</code><br><code>b = 000...0001</code></p><p>Adding these two numbers will generate a carry for each digit addition, resulting in a chained carry propagation from the least significant digit to the most significant digit. For instance, if we group <code>k = 2</code> digits together, where:<br><code>a' = 99 99 ... 99 99 99</code><br><code>b' = 00 00 ... 00 00 01</code></p><p>The addition in the first phase without accounting for carries might yield:<br><code>Partial Sum = 0, 0, 0, ..., 0, 0, 100</code></p><p>Here, the carries generated would be:<br><code>Carries = 0, 0, 0, ..., 0, 1, 0</code></p><p>To obtain the correct result, we need to propagate the carry in a chained manner and adjust the partial sum accordingly. This chained propagation involves processing carries sequentially, which can degrade performance if not handled efficiently.</p><h3 id=quantifying-the-likelihood-of-chained-carry-propagation-in-addition>Quantifying the Likelihood of Chained Carry Propagation in Addition<a hidden class=anchor aria-hidden=true href=#quantifying-the-likelihood-of-chained-carry-propagation-in-addition>#</a></h3><p>To understand how frequently such problematic scenarios occur, we can analyze the probability of encountering such cases mathematically.</p><p><strong>Probability Analysis:</strong></p><p>Assume we are working with <code>n</code>-digit numbers, and we group <code>k</code> digits together into a single 32-bit or 64-bit block. The probability of chained carry propagation depends on the likelihood that the partial sums from the first phase will generate additional carries when processed in the second phase. An analysis of how likely those cases will occur will be posted here shortly!</p><h3 id=algorithm-of-addition-implementation>Algorithm of Addition Implementation<a hidden class=anchor aria-hidden=true href=#algorithm-of-addition-implementation>#</a></h3><p>For our approach, Algorithm 1 illustrates the process of adding two large numbers represented as arrays of digits, where each index packs nine digits together. We use 32-bit storage for each index, and while it can technically hold up to 10 digits, we&rsquo;ve limited it to nine digits to account for overflow.</p><p>The algorithm operates in two distinct phases:</p><ol><li><strong>First Phase (Addition without Carry Propagation):</strong> In this phase, we add the grouped digits from both arrays without waiting for the carry to be propagated. If the resulting sum at any position exceeds nine digits (i.e., 10^9 or more), we generate a carry for that position and subtract 10^9 from the sum. This adjustment ensures that the sum remains within nine digits. The reason for setting the carry array and adjusting the sum in this way is that adding two nine-digit numbers will never result in more than 10 digits, so any overflow is correctly handled by setting the carry.</li></ol><ul><li><strong>Carry Array Left Shift:</strong> Before proceeding to the second phase, the carry array is left-shifted by one position. This shift is necessary because the carry generated in the current partial sum needs to be propagated to the subsequent partial sums in the next phase.</li></ul><ol start=2><li><strong>Second Phase (Carry Propagation):</strong> In this phase, we add the carries generated during the first phase to the corresponding partial sums. This ensures that the final result accounts for all necessary carries and produces the correct sum.</li></ol><p><strong>Note:</strong> Since each index stores nine digits, the likelihood of generating additional carries during the second phase is relatively low, as discussed earlier. However, it is crucial to have a mechanism in place to check and handle any remaining carry propagation to ensure the correctness of the algorithm.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-6-1><a class=lnlinks href=#hl-6-1> 1</a>
</span><span class=lnt id=hl-6-2><a class=lnlinks href=#hl-6-2> 2</a>
</span><span class=lnt id=hl-6-3><a class=lnlinks href=#hl-6-3> 3</a>
</span><span class=lnt id=hl-6-4><a class=lnlinks href=#hl-6-4> 4</a>
</span><span class=lnt id=hl-6-5><a class=lnlinks href=#hl-6-5> 5</a>
</span><span class=lnt id=hl-6-6><a class=lnlinks href=#hl-6-6> 6</a>
</span><span class=lnt id=hl-6-7><a class=lnlinks href=#hl-6-7> 7</a>
</span><span class=lnt id=hl-6-8><a class=lnlinks href=#hl-6-8> 8</a>
</span><span class=lnt id=hl-6-9><a class=lnlinks href=#hl-6-9> 9</a>
</span><span class=lnt id=hl-6-10><a class=lnlinks href=#hl-6-10>10</a>
</span><span class=lnt id=hl-6-11><a class=lnlinks href=#hl-6-11>11</a>
</span><span class=lnt id=hl-6-12><a class=lnlinks href=#hl-6-12>12</a>
</span><span class=lnt id=hl-6-13><a class=lnlinks href=#hl-6-13>13</a>
</span><span class=lnt id=hl-6-14><a class=lnlinks href=#hl-6-14>14</a>
</span><span class=lnt id=hl-6-15><a class=lnlinks href=#hl-6-15>15</a>
</span><span class=lnt id=hl-6-16><a class=lnlinks href=#hl-6-16>16</a>
</span><span class=lnt id=hl-6-17><a class=lnlinks href=#hl-6-17>17</a>
</span><span class=lnt id=hl-6-18><a class=lnlinks href=#hl-6-18>18</a>
</span><span class=lnt id=hl-6-19><a class=lnlinks href=#hl-6-19>19</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>Algorithm 1: GROUPED_ADD
</span></span><span class=line><span class=cl>Preprocessing:
</span></span><span class=line><span class=cl>  a, b → unsigned 32-bit integer arrays, each index packed with 9 digits, of length n.
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>1st Phase: Perform additions, without halting for carry generation:
</span></span><span class=line><span class=cl>for i = 0 to n:
</span></span><span class=line><span class=cl> sum[i] ← a[i] + b[i]
</span></span><span class=line><span class=cl> if sum[i] ≥ 10^9:
</span></span><span class=line><span class=cl>   carry_array[i] ← 1
</span></span><span class=line><span class=cl>   sum[i] ← sum[i] - 10^9
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>LeftShift(carry_array, 1): Fill LSB with 0
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>2nd Phase: Perform the carry propagation:
</span></span><span class=line><span class=cl>for i = 0 to n:
</span></span><span class=line><span class=cl>  sum[i] ← sum[i] + carry_array[i]
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>Check if carry still persists, and if so, handle carry propagation normally. 
</span></span><span class=line><span class=cl>Return the array sum.   
</span></span></code></pre></td></tr></table></div></div><p>Now that we understand the basic mechanism of our addition algorithm, we can observe that each addition within both the two phases is independent of each other; it is just that phase two needs to wait until phase one has been completed. Using this observation, we can simultaneously process the addition within each phase, leveraging multiple operations processed at once. Therefore, SIMD (Single Instruction, Multiple Data) can be leveraged for data parallelism, while multi-threading can be used to implement task parallelism. Additionally, other techniques, such as GPU acceleration, may be applied to further exploit parallelism in the process of the additions.</p><p>Algorithm 2, below, illustrates how SIMD (Single Instruction, Multiple Data) can be utilized to perform simultaneous additions of grouped digits. By using 32-bit storage for each group of digits and leveraging 512-bit SIMD registers, we can pack up to 16 groups of digits into a single vector register. This configuration enables us to perform arithmetic operations on 16 groups of digits in parallel.</p><p>Specifically, since each group contains nine digits, the SIMD register can handle arithmetic operations on 16*9 = 144 digits simultaneously. This capability significantly reduces the overall execution time of the addition operation by processing multiple groups of digits in parallel.</p><p>The use of SIMD allows us to exploit parallelism to accelerate computations, which is particularly beneficial for efficiently handling large numbers. This approach enhances performance by minimizing the number of iterations needed to process all the digits.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-7-1><a class=lnlinks href=#hl-7-1> 1</a>
</span><span class=lnt id=hl-7-2><a class=lnlinks href=#hl-7-2> 2</a>
</span><span class=lnt id=hl-7-3><a class=lnlinks href=#hl-7-3> 3</a>
</span><span class=lnt id=hl-7-4><a class=lnlinks href=#hl-7-4> 4</a>
</span><span class=lnt id=hl-7-5><a class=lnlinks href=#hl-7-5> 5</a>
</span><span class=lnt id=hl-7-6><a class=lnlinks href=#hl-7-6> 6</a>
</span><span class=lnt id=hl-7-7><a class=lnlinks href=#hl-7-7> 7</a>
</span><span class=lnt id=hl-7-8><a class=lnlinks href=#hl-7-8> 8</a>
</span><span class=lnt id=hl-7-9><a class=lnlinks href=#hl-7-9> 9</a>
</span><span class=lnt id=hl-7-10><a class=lnlinks href=#hl-7-10>10</a>
</span><span class=lnt id=hl-7-11><a class=lnlinks href=#hl-7-11>11</a>
</span><span class=lnt id=hl-7-12><a class=lnlinks href=#hl-7-12>12</a>
</span><span class=lnt id=hl-7-13><a class=lnlinks href=#hl-7-13>13</a>
</span><span class=lnt id=hl-7-14><a class=lnlinks href=#hl-7-14>14</a>
</span><span class=lnt id=hl-7-15><a class=lnlinks href=#hl-7-15>15</a>
</span><span class=lnt id=hl-7-16><a class=lnlinks href=#hl-7-16>16</a>
</span><span class=lnt id=hl-7-17><a class=lnlinks href=#hl-7-17>17</a>
</span><span class=lnt id=hl-7-18><a class=lnlinks href=#hl-7-18>18</a>
</span><span class=lnt id=hl-7-19><a class=lnlinks href=#hl-7-19>19</a>
</span><span class=lnt id=hl-7-20><a class=lnlinks href=#hl-7-20>20</a>
</span><span class=lnt id=hl-7-21><a class=lnlinks href=#hl-7-21>21</a>
</span><span class=lnt id=hl-7-22><a class=lnlinks href=#hl-7-22>22</a>
</span><span class=lnt id=hl-7-23><a class=lnlinks href=#hl-7-23>23</a>
</span><span class=lnt id=hl-7-24><a class=lnlinks href=#hl-7-24>24</a>
</span><span class=lnt id=hl-7-25><a class=lnlinks href=#hl-7-25>25</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>Algorithm 2: SIMD_GROUPED_ADD
</span></span><span class=line><span class=cl>Preprocessing:
</span></span><span class=line><span class=cl>  a, b → unsigned 32-bit integer arrays, each index packed with 9 digits group, of length n.
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>Assumption:
</span></span><span class=line><span class=cl>  a_vec, b_vec, c_vec, sum_vec are vector registers of size 512 bits, to be processed by SIMD instructions.
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>1st Phase: Perform additions, without halting for carry generation:
</span></span><span class=line><span class=cl>for i = 0 to n increment by 16:
</span></span><span class=line><span class=cl>  a_vec   ← Accumulate 16 elements from (a + i)
</span></span><span class=line><span class=cl>  b_vec   ← Accumulate 16 elements from (b + i)
</span></span><span class=line><span class=cl>  sum_vec ← a_vec + b_vec
</span></span><span class=line><span class=cl>  for j = 0 to 16:
</span></span><span class=line><span class=cl>    if sum_vec[j] ≥ 10^9:
</span></span><span class=line><span class=cl>      carry_array[i + j] ← 1
</span></span><span class=line><span class=cl>      sum_vec[j] ← sum_vec[j] - 10^9
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>LeftShift(carry_array, 1): Fill LSB with 0
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>2nd Phase: Perform the carry propagation:
</span></span><span class=line><span class=cl>for i = 0 to n increment by 16:
</span></span><span class=line><span class=cl>  sum_vec ← sum_vec + carry_array
</span></span><span class=line><span class=cl>If carry still persists, handle normally
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>Return the array sum.
</span></span></code></pre></td></tr></table></div></div><p>Overall, these algorithms are designed to minimize the effects of carry propagation by grouping digits in a manner that reduces the probability of cascading carry dependencies. By handling independent operations in parallel, we can achieve significant performance improvements when adding large numbers, making these approaches well-suited for high-performance computing environments.</p><h3 id=performance-bottleneck-of-addition>Performance Bottleneck of Addition<a hidden class=anchor aria-hidden=true href=#performance-bottleneck-of-addition>#</a></h3><p>When chained carry propagation occurs, carries generated from the one end of partial sums must be propagated to the other end of partial sums. This process can significantly degrade overall performance and impact parallelization, as it may require sequentially waiting for the carry to be generated for preceding partial sums. This introduces a significant dependency that can halt the parallel processing, thereby affecting performance.
In our implementations, we&rsquo;ve stuck to two phases of parallel processing of arithmetic operations, followed by a handle that checks for if further carry propagation is needed; if that is the case, we handle those cases simply by conventional sequential method of starting from the last group of partial sums to the most significant end. This hampers the overall performance of the implementation, but it ensures correctness in those scenarios.</p><p>Another alternative is to speculate or precompute the carries. However, it is important to analyze whether the cost of carry speculation justifies its benefits, as chained carry propagation typically affects only a limited number of cases.</p><p>Despite these potential drawbacks, our approach speeds up the addition process overall. Since cases requiring extensive carry propagation are relatively rare, the benefits of enhanced performance in most scenarios outweigh the occasional performance hit from sequential handling.</p><h3 id=implementation-of-addition>Implementation of Addition<a hidden class=anchor aria-hidden=true href=#implementation-of-addition>#</a></h3><p>The implementation of Algorithm 1 is available here:
<a href=https://github.com/iamsubhrajit10/Large-Number-Arithmetic-Operations/blob/main/Addition-Implementation/add_limb.c>github-group-add</a>. The implementation of Algorithm 2 can be found here <a href=https://github.com/iamsubhrajit10/Large-Number-Arithmetic-Operations/blob/main/Addition-Implementation/add_limb_avx.c>github-simd-add</a>. Both implementations are written in C, with AVX512 intrinsics used for SIMD operations on Intel architectures.</p><p>Please note that these implementations may require modifications and will be updated periodically. Further details about the implementation will be provided soon!</p><h3 id=gmps-mpz_add>GMP&rsquo;s mpz_add<a hidden class=anchor aria-hidden=true href=#gmps-mpz_add>#</a></h3><p>To be posted here shortly!</p><h4 id=gmps-mpz_add-function-disassembled>GMP&rsquo;s mpz_add Function Disassembled<a hidden class=anchor aria-hidden=true href=#gmps-mpz_add-function-disassembled>#</a></h4><p>To be posted here shortly!</p><h4 id=explicitly-vectorized-addition-disassembled>Explicitly Vectorized Addition Disassembled<a hidden class=anchor aria-hidden=true href=#explicitly-vectorized-addition-disassembled>#</a></h4><p>To be posted here shortly!</p><h3 id=performance-comparison-of-addition-with-gmp>Performance Comparison of Addition with GMP<a hidden class=anchor aria-hidden=true href=#performance-comparison-of-addition-with-gmp>#</a></h3><p>To be posted here shortly!</p><h2 id=subtraction>Subtraction<a hidden class=anchor aria-hidden=true href=#subtraction>#</a></h2><p>Details about the subtraction implementation will be posted here shortly!</p><h3 id=algorithm-of-subtraction-implementation>Algorithm of Subtraction Implementation<a hidden class=anchor aria-hidden=true href=#algorithm-of-subtraction-implementation>#</a></h3><p>To be posted here shortly!</p><h3 id=implementation-of-subtraction-using-simd>Implementation of Subtraction using SIMD<a hidden class=anchor aria-hidden=true href=#implementation-of-subtraction-using-simd>#</a></h3><p>You can find the implementation of the subtraction algorithm here: <a href=https://github.com/iamsubhrajit10/Large-Number-Arithmetic-Operations/blob/subtraction/subtraction/sub.c>github-explicit-vectorized</a>. The implementation is written in C, with explicit vectorization using AVX512 intrinsics for Intel architectures.</p><h3 id=gmps-mpz_sub>GMP&rsquo;s mpz_sub<a hidden class=anchor aria-hidden=true href=#gmps-mpz_sub>#</a></h3><h4 id=gmps-mpz_sub-function-disassembled>GMP&rsquo;s mpz_sub Function Disassembled<a hidden class=anchor aria-hidden=true href=#gmps-mpz_sub-function-disassembled>#</a></h4><h4 id=explicitly-vectorized-subtraction-disassembled>Explicitly Vectorized Subtraction Disassembled<a hidden class=anchor aria-hidden=true href=#explicitly-vectorized-subtraction-disassembled>#</a></h4><h3 id=performance-comparison-of-subtraction-with-gmp>Performance Comparison of Subtraction with GMP<a hidden class=anchor aria-hidden=true href=#performance-comparison-of-subtraction-with-gmp>#</a></h3><p>Below are some of the results of our explicitly vectorized subtraction implementation for Intel Skylake architecture, comparing the performance of the manual vectorized code with the GMP&rsquo;s mpz_sub function. The results are presented in terms of the number of operations per second.</p><h4 id=rdtsc-random-avx-vs-gmp---subtraction>RDTSC Random AVX vs GMP - Subtraction<a hidden class=anchor aria-hidden=true href=#rdtsc-random-avx-vs-gmp---subtraction>#</a></h4><p><img loading=lazy src=/images/rdtsc_random_avx-vs-gmp.png alt="Subtraction Results - 1" title="RDTSC Random AVX vs GMP"></p><h4 id=timespec-random-avx-vs-gmp---subtraction>Timespec Random AVX vs GMP - Subtraction<a hidden class=anchor aria-hidden=true href=#timespec-random-avx-vs-gmp---subtraction>#</a></h4><p><img loading=lazy src=/images/timespec_random_avx-vs-gmp.png alt="Subtraction Results - 2" title="TIMESPEC Random AVX vs GMP"></p><h4 id=rusage-ru_utime-random-avx-vs-gmp---subtraction>RUSAGE (ru_utime) Random AVX vs GMP - Subtraction<a hidden class=anchor aria-hidden=true href=#rusage-ru_utime-random-avx-vs-gmp---subtraction>#</a></h4><p><img loading=lazy src=/images/rusage_random_avx-vs-gmp.png alt="Subtraction Results - 3" title="RUSAGE Random AVX vs GMP"></p><h2 id=multiplication>Multiplication<a hidden class=anchor aria-hidden=true href=#multiplication>#</a></h2><p>To be posted shortly!</p><h3 id=grade-school-multiplication>Grade-school Multiplication<a hidden class=anchor aria-hidden=true href=#grade-school-multiplication>#</a></h3><p>To be posted shortly!</p><h3 id=urdhva-tiryagbhyam>Urdhva-Tiryagbhyam<a hidden class=anchor aria-hidden=true href=#urdhva-tiryagbhyam>#</a></h3><p>We chose Urdhva-Tiryagbhyam due to its efficient handling of digits, which can be particularly beneficial for large number multiplication. It also offers caching benefits that align with our performance goals.</p><h3 id=karatsuba>Karatsuba<a hidden class=anchor aria-hidden=true href=#karatsuba>#</a></h3><p>To be posted shortly!</p><h3 id=toom-cook>Toom-Cook<a hidden class=anchor aria-hidden=true href=#toom-cook>#</a></h3><p>To be posted shortly!</p><h3 id=fft>FFT<a hidden class=anchor aria-hidden=true href=#fft>#</a></h3><p>To be posted shortly!</p></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=prev href=https://iamsubhrajit10.me/posts/about-me/><span class=title>« Prev</span><br><span>About Me</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share DigitsOnTurbo: Optimizing Large Number Arithmetic Operations on x" href="https://x.com/intent/tweet/?text=DigitsOnTurbo%3a%20Optimizing%20Large%20Number%20Arithmetic%20Operations&amp;url=https%3a%2f%2fiamsubhrajit10.me%2fposts%2fdigitsonturbo%2f&amp;hashtags="><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share DigitsOnTurbo: Optimizing Large Number Arithmetic Operations on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fiamsubhrajit10.me%2fposts%2fdigitsonturbo%2f&amp;title=DigitsOnTurbo%3a%20Optimizing%20Large%20Number%20Arithmetic%20Operations&amp;summary=DigitsOnTurbo%3a%20Optimizing%20Large%20Number%20Arithmetic%20Operations&amp;source=https%3a%2f%2fiamsubhrajit10.me%2fposts%2fdigitsonturbo%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share DigitsOnTurbo: Optimizing Large Number Arithmetic Operations on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fiamsubhrajit10.me%2fposts%2fdigitsonturbo%2f&title=DigitsOnTurbo%3a%20Optimizing%20Large%20Number%20Arithmetic%20Operations"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share DigitsOnTurbo: Optimizing Large Number Arithmetic Operations on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fiamsubhrajit10.me%2fposts%2fdigitsonturbo%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share DigitsOnTurbo: Optimizing Large Number Arithmetic Operations on whatsapp" href="https://api.whatsapp.com/send?text=DigitsOnTurbo%3a%20Optimizing%20Large%20Number%20Arithmetic%20Operations%20-%20https%3a%2f%2fiamsubhrajit10.me%2fposts%2fdigitsonturbo%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share DigitsOnTurbo: Optimizing Large Number Arithmetic Operations on telegram" href="https://telegram.me/share/url?text=DigitsOnTurbo%3a%20Optimizing%20Large%20Number%20Arithmetic%20Operations&amp;url=https%3a%2f%2fiamsubhrajit10.me%2fposts%2fdigitsonturbo%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share DigitsOnTurbo: Optimizing Large Number Arithmetic Operations on ycombinator" href="https://news.ycombinator.com/submitlink?t=DigitsOnTurbo%3a%20Optimizing%20Large%20Number%20Arithmetic%20Operations&u=https%3a%2f%2fiamsubhrajit10.me%2fposts%2fdigitsonturbo%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://iamsubhrajit10.me>Subhrajit Das</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>